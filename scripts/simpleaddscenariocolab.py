# -*- coding: utf-8 -*-
"""SimpleADDScenarioColab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yBHXw30_rc1E3NI6g8NsvbZ54_t1DMXX

Retrieve/Import Necessary Packages
"""

import sys
sys.path.append('/usr/local/lib/python3.7/site-packages/')
import os

import math
import numpy as np
np.random.seed(0)
import pandas as pd
import deepchem as dc
from deepchem.utils.save import load_from_disk
from deepchem.data import data_loader
import pickle
import matplotlib.pyplot as plt
import seaborn as sns


"""
Prepare Data and Model Definitions

Load and Featurize Data
"""
dataset_file = "./enamineSubset100KGroundTruth.csv"
ground_truth_dataset = pd.read_csv(dataset_file)
low_bace_dataset = ground_truth_dataset.sort_values(by="bace")[ : len(ground_truth_dataset)//4 ] #take quarter worst binder potential starters

top_5_percent_index = len(ground_truth_dataset) // 20
top_5_percent_bace_cutoff = ground_truth_dataset.sort_values(by="bace", ascending=False)["bace"].tolist()[top_5_percent_index]

###featurized ground truth for scoring
featurizer = dc.feat.ConvMolFeaturizer()
loader = dc.data.CSVLoader(tasks=["bace", "esol", "logD"], smiles_field="SMILES", featurizer=featurizer)
dataset_feat = loader.featurize(dataset_file) #featurize the molecules from the ground truth dataset
transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)
ground_truth_for_scoring = transformer.transform(dataset_feat)


"""Define Main Experimenter Model"""

###define Abstract Data Type to hold search information, including ensemble

class Experimenter():
    """Class representing a research scientist/team going through the drug development process.
    
    Parameters
    ----------
    N : int
        Number of samples to initially train the experimenter ensemble on.
    M : int
        Number of molecules to purchase in each batch.
    ensemble_size : int, optional
        Number of models in experimenter ensemble.
    epochs : int, optional
        Number of epochs to train ensemble models for at each stage.
    molecule_cost : int or float, optional
        Monetary cost of purchasing a single molecule.
    target_bounds : dictionary of str:tuples(floats), optional
        Desired range for each property.
    sampling_mode : string {"thompson", "highest_mean", "random"}
        The means of choosing the ensemble outputs/molecules.
    
    Attributes
    ----------
    ensemble : dictionary of deepchem.models.GrachConvModel
        Models representing the experimenter knowledge/predictions and uncertainty.
    history : list of dictionaries storing model attributes
        Snapshots of the model state at each time step.
    samples_seen : pandas.DataFrame
        Ground truth values of the molecules seen before. Includes initial training set.
    smiles_seen : list of str
        SMILES strings of the molecules seen before.
    selected_prediction : pandas.DataFrame
        The molecule values used to make the next decision.
    all_predictions : dict<int,pandas.DataFrame>
        Predicted values of entire ensemble at this time step. Ensemble model keys (random seeds) map to model's prediction.
    cost : int or float
        Total monetary cost incurred at the current time.
    number_molecules : int
        Total number of molecules purchased at the current time.
    time : int
        Total number of days spent up to the current time.
        
    """
    def __init__(self, N, M, ensemble_size=3, epochs=10, molecule_cost=200,
                 target_bounds={"bace":(4, math.inf), "esol":(-5, math.inf), "logD":(-0.4, 5.6)}, sampling_method="highest_mean"):
        self.N = N #initial samples
        self.M = M #batch size
        self.ensemble_size = ensemble_size
        self.epochs = epochs
        self.molecule_cost = molecule_cost
        self.target_bounds = target_bounds
        if sampling_method == "thompson" or sampling_method == "highest_mean" or sampling_method == "random":
            self.sampling_method = sampling_method
        else:
            raise ValueError("Input for sampling method was not allowed argument. Choices are thompson, highest_mean, and random.")
        
        self.ensemble = {i:dc.models.GraphConvModel(n_tasks=3, mode='regression', batch_size=20, random_seed=i, tensorboard=True) 
                         for i in range(self.ensemble_size)} #map each model to its seed
        self.history = [] #save snapshot of model, on disk
        self.samples_seen = None
        self.smiles_seen = []
        self.selected_prediction = pd.DataFrame()
        self.all_predictions = {}
        self.cost = 0
        self.number_molecules = 0
        self.time = 0 #days
        
        
    def train_model(self, model, dataset):
        """Helper function to train a given ensemble model on a given dataset.
        
        Parameters
        ----------
        model : Keras model (generally deepchem.GraphConvModel)
            Model to be trained.
        dataset : pandas.DataFrame
            Dataset to train on. Must include "SMILES", "bace", "esol", and "logD" headers.
            
        """
        #convert DataFrame to CSV and read in as deepchem.Dataset via deepchem.CSVLoader
        
        dataset.to_csv("training_dataset.csv")
        
        featurizer = dc.feat.ConvMolFeaturizer()
        loader = dc.data.CSVLoader(tasks=["bace", "esol", "logD"], smiles_field="SMILES", featurizer=featurizer)

        dataset_feat = loader.featurize("training_dataset.csv")
        
        transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)
        dataset_feat = transformer.transform(dataset_feat)

        model.fit(dataset_feat, nb_epoch=self.epochs, deterministic=True, restore=False)
    
    
    def train_ensemble(self, dataset):
        """Helper function to train model ensemble.
        
        Parameters
        ----------
        dataset : pandas.Dataset
            Dataset on which to train models. Must include "SMILES", "bace", "esol", and "logD" headers.
        
        """
        for model in self.ensemble.values():
            self.train_model(model, dataset)

    
    def initial_training(self, verbose=False):
        """Train model ensemble for the first time on self.N samples randomly chosen from the 2500 lowest bace affinity-scored 
        molecules.
        
        Parameters
        ----------
        verbose : bool
            Whether to print progress updates.
        
        Notes
        -----
        If self.N > 1/4 of dataset, ensemble will be trained on 2500 samples.
        Records first history object.
        
        """
        idx_range = self.N if self.N < low_bace_dataset.shape[0] else low_bace_dataset.shape[0]
        rand_indices = np.random.choice(range(low_bace_dataset.shape[0]), idx_range, replace=False) #select random row indices
        
        init_ensemble_dataset = pd.DataFrame()
        for idx in rand_indices:
            init_ensemble_dataset = init_ensemble_dataset.append( low_bace_dataset.iloc[idx], ignore_index=True )
        
        if verbose:
            print("Training set selected.")
            
        self.samples_seen = init_ensemble_dataset ### collect the examples seen during initial training (ground truth values)
        self.smiles_seen = init_ensemble_dataset["SMILES"].tolist()
        
        #cost/time to initially train? free initial knowledge?
        self.cost += self.molecule_cost * len(init_ensemble_dataset)
        self.number_molecules += len(init_ensemble_dataset)
        self.time = 0
        

        if self.sampling_method != "random":
            if verbose:
                print("Training ensemble...")
            self.train_ensemble(init_ensemble_dataset) #train ensemble on initial dataset, unless we are randomly sampling and do not need to        
            if verbose:
                print("Ensemble trained.")
                
        self.record_history()

                
    def get_component_score(self, arr, keys):
        """Helper function to get the scaled "goodness" of the input scores.
        
        Parameters
        ----------
        array : numpy.array
             Array with bace, esol, and logD scores.
        keys : collection of strings from {"bace", "esol", "logD"}
            Which scores to incorporate into the overall goodness.
        
        Returns
        -------
        numpy.array
            Sum of component scores.
        
        """
        scores = []
        if "bace" in keys:
            #higher bace => higher score
            bace = arr[:,0]
            bace_range = self.target_bounds["bace"]
            scores.append( np.where(bace < bace_range[0], 0.2*bace-0.8, 0.05*bace-0.2) )
            #dec penalty when score>low end of range
        
        if "esol" in keys:
            esol = arr[:,1]
            esol_range = self.target_bounds["esol"]
            scores.append( np.where(esol < esol_range[0], esol - np.absolute(esol-esol_range[1])**2, esol) )
        
        if "logD" in keys:
            #logD within range is not penalized
            logD = arr[:,2]
            logD_range = self.target_bounds["logD"]
            #handle lower end of range
            int_arr = np.where(logD < logD_range[0], logD - np.absolute(logD-logD_range[0]), logD)
            #handle upper end of range
            scores.append(np.where(int_arr > logD_range[1], int_arr - np.absolute(int_arr-logD_range[1]), int_arr) )

        return sum(scores)
        
    
    def score_and_select_top(self):
        """Scores all molecules and selects the top M for "purchase".
        
        """
        if self.sampling_method == "highest_mean":
            #generate and store all predictions
             predicted = np.zeros( (len(ground_truth_for_scoring),3) )
            for key in self.ensemble.keys():
                pred = self.ensemble[key].predict(ground_truth_for_scoring)
                pred = transformer.untransform(pred) #undo normalization on outputs
                predicted += pred #sum model predictions
                self.all_predictions[key] = self.prediction_array_to_dataframe(pred) #store each prediction as a labeled dataframe
            predicted /= len(self.ensemble) #avg model predictions
            results_df = self.prediction_array_to_dataframe(predicted)
            
            
        elif self.sampling_method == "thompson":
            #generate and store all predictions
            for key in self.ensemble.keys():
                pred = self.ensemble[key].predict(ground_truth_for_scoring)
                pred = transformer.untransform(pred) #undo normalization on outputs
                self.all_predictions = { key : self.prediction_array_to_dataframe( pred ) } #store all labeled dataframes       

            
            #Thompson sampling
            results_df = pd.DataFrame()
            for row_idx in range( len(ground_truth_for_scoring) ):
                pred_key = np.random.randint(low=0, high=len(self.ensemble)) #select one random prediction array to select a row from
                pred_df = self.all_predictions[pred_key]
                pred_row = pred_df.iloc[[row_idx]]
                results_df = pd.concat([results_df, pred_row], sort=False)         
        
        
        elif self.sampling_method == "random":
            ###randomly select up to M points from those not seen
            unseen = ground_truth_dataset.loc[~ground_truth_dataset['SMILES'].isin(self.smiles_seen)] #remove prev seen
            unseen = unseen.iloc[np.random.permutation(len(unseen))] #shuffle remaining samples
            unseen = unseen[:self.M] if (len(unseen) > self.M) else unseen #select up to self.M samples
            
            self.samples_seen = pd.concat([self.samples_seen,unseen], sort=False)
            self.smiles_seen = self.samples_seen["SMILES"].tolist()
            self.cost += self.molecule_cost * len(unseen)
            self.number_molecules += len(unseen)
            self.time += 28 #4 weeks to buy and experiment             
            return 
            
        self.selected_prediction = results_df #also store the dataframe with the data we chose to make decisions with
        
        unseen_predicted_rows = results_df.loc[~results_df['SMILES'].isin(self.smiles_seen)] #also remove predicted values previously seen
        unseen_predicted_rows = unseen_predicted_rows.sort_values(by="goodness", ascending=False) #sort predictions with highest goodness at top
        
        predicted_subset = unseen_predicted_rows[:self.M] if (len(unseen_predicted_rows) > self.M) else unseen_predicted_rows #select up to self.M samples from the predictions
        predicted_subset_smiles = predicted_subset["SMILES"].tolist()
        
        new_batch_ground_truth = ground_truth_dataset.loc[ground_truth_dataset['SMILES'].isin(predicted_subset_smiles)]
        
        self.samples_seen = pd.concat([self.samples_seen,new_batch_ground_truth], sort=False)
        self.smiles_seen = self.samples_seen["SMILES"].tolist()
        self.cost += self.molecule_cost * len(new_batch_ground_truth)
        self.number_molecules += len(new_batch_ground_truth)
        self.time += 28 #4 weeks to buy and experiment
    
    
    def prediction_array_to_dataframe(self, array):
        #copy SMILES and assign calculated scores, store in self.predictions
        df = pd.DataFrame()
        df["SMILES"] = ground_truth_dataset["SMILES"]   
        goodness = self.get_component_score(array, ["bace", "esol", "logD"])
        df["bace"] = array[:,0]
        df["esol"] = array[:,1]
        df["logD"] = array[:,2]
        df["goodness"] = goodness
        return df
        
    
    def record_history(self):
        """Stores model costs and experience for later analysis.
        
        Notes
        -----
        Does not save self.history attribute, in order to avoid redundantly storing the data in it.
        Only saves attributes that change in each time step.
        
        """
        hist = {}
        hist["samples_seen"] = self.samples_seen
        hist["smiles_seen"] = self.smiles_seen
        hist["cost"] = self.cost
        hist["number_molecules"] = self.number_molecules
        hist["time"] = self.time
        hist["selected_prediction"] = self.selected_prediction
        hist["all_predictions"] = self.all_predictions
        self.history.append(hist)
     

    def run(self):
        """Simple wrapper to automate calls to select molecules and update models. 
        
        Returns
        -------
        candidates : pandas.DataFrame
            The candidate compounds that satisfy the given criteria.

        Notes
        -----
        Must be preceded by initial training of model ensemble.
        
        """       
        itr = 0
        while len(self.samples_seen) < len(ground_truth_dataset): #search entire database, with early stopping              
            self.score_and_select_top()
            self.record_history()

            print("PROGRESS:",len(self.samples_seen),"of",len(ground_truth_dataset))
            if self.sampling_method != "random":
                self.train_ensemble(self.samples_seen)
                
            with open(f"{self.sampling_method}_model_{itr}.pickle", "wb") as f:
                pickle.dump(self.history,f)
            itr += 1

"""#Run the Model and Obtain Data"""

#N = [96, 384, 1536] #initial train set size
N = [384]
#M = [96, 384, 1536] #batch size -> 96 wells, multiples
M = [384]

number_reps = 10

for method in ("highest_mean", "thompson", "random"):
    for i in range(number_reps):
        os.chdir(f"./{method}_models/{i}/")

        print("\n","Iteration:",i,"\n")
        model = Experimenter(n, m, ensemble_size=5, epochs=10, sampling_method=method)
        model.initial_training()
        model.run()