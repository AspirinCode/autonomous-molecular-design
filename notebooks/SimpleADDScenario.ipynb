{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff bace score for 95th percentile: 4.870083\n",
      "Columns of dataset: ['Index' 'SMILES' 'bace' 'esol' 'logD']\n",
      "Number of examples in dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "###initialize imports and dataset\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import deepchem as dc\n",
    "from deepchem.utils.save import load_from_disk\n",
    "from deepchem.data import data_loader\n",
    "import random\n",
    "random.seed(0)\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset_file = \"./enamineSubset10KGroundTruth.csv\"\n",
    "ground_truth_dataset = load_from_disk(dataset_file) #pandas Dataframe\n",
    "\n",
    "low_bace_dataset = ground_truth_dataset.sort_values(by=\"bace\")[:2500] #take 2.5K worst binder potential starters,shouldn't need copy\n",
    "\n",
    "top_5_percent_index = len(ground_truth_dataset) // 20\n",
    "top_5_percent_bace_cutoff = ground_truth_dataset.sort_values(by=\"bace\", ascending=True,)[\"bace\"][top_5_percent_index]\n",
    "\n",
    "print(\"Cutoff bace score for 95th percentile:\", top_5_percent_bace_cutoff)\n",
    "print(\"Columns of dataset: %s\" % str(ground_truth_dataset.columns.values))\n",
    "print(\"Number of examples in dataset: %s\" % str(ground_truth_dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###initialize ground truth models and methods to access them\n",
    "\n",
    "def load_oracle_models():\n",
    "    \"\"\"Loads the pretrained ground truth models for evaluating molecules' properties on-the-fly.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    oracle : dict\n",
    "        A dictionary containing models mapped to their property keywords: \"bace\", \"esol\", \"logD\".\n",
    "    \"\"\"\n",
    "    bace_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/bace\")\n",
    "    esol_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/esol\")\n",
    "    logD_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/logD\")\n",
    "    bace_model.restore()\n",
    "    esol_model.restore()\n",
    "    logD_model.restore()\n",
    "    oracle = {\"bace\":bace_model, \"esol\":esol_model, \"logD\":logD_model} #get each model via the named property\n",
    "    return oracle\n",
    "\n",
    "def query_oracle(dataset, oracle):\n",
    "    \"\"\"Evaluate molecules on-the-fly for their estimated bace, esol, and logD scores.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pandas.DataFrame\n",
    "        The input dataset; must includes a field with smiles strings under keyword \"SMILES\".\n",
    "    oracle : dictionary( dc.models.GraphConvModel )\n",
    "        The pretrained ground truth value prediction models.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    results : pandas.DataFrame\n",
    "        Copy of input dataset with newly estimated bace, esol, and logD scores under those headers. \n",
    "    \"\"\"\n",
    "    query_file = \"./temp/oracle_eval.csv\"\n",
    "    dataset.to_csv(query_file)\n",
    "    \n",
    "    results = dataset.copy(deep=True) #defensive copy of input dataframe \n",
    "    \n",
    "    featurizer = dc.feat.ConvMolFeaturizer()\n",
    "    for prop in (\"bace\", \"esol\", \"logD\"):\n",
    "        #retrieve appropriate model from oracle\n",
    "        model = oracle[prop]\n",
    "        \n",
    "        #load, featurize, and normalize input dataset\n",
    "        loader = dc.data.CSVLoader(tasks=[prop], smiles_field=\"SMILES\",featurizer=featurizer)\n",
    "        dataset_feat = loader.featurize(query_file)\n",
    "        transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "        dataset_feat = transformer.transform(dataset_feat)\n",
    "        \n",
    "        #predict and assign property results to keyword\n",
    "        predicted = model.predict(dataset_feat)\n",
    "        results[prop] = predicted\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "###define Abstract Data Type to hold search information, including ensemble\n",
    "\n",
    "class Experimenter():\n",
    "    \"\"\"Class representing a research scientist/team going through the drug development process.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        Number of samples to initially train the experimenter ensemble on.\n",
    "    M : int\n",
    "        Number of molecules to purchase in each batch.\n",
    "    ensemble_size : int, optional\n",
    "        Number of models in experimenter ensemble.\n",
    "    epochs : int, optional\n",
    "        Number of epochs to train ensemble models for at each stage.\n",
    "    molecule_cost : int or float, optional\n",
    "        Monetary cost of purchasing a single molecule.\n",
    "    target_bounds : dictionary of str:tuples(floats), optional\n",
    "        Desired range for each property.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    ensemble : dictionary of deepchem.models.GrachConvModel\n",
    "        Models representing the experimenter knowledge/predictions and uncertainty.\n",
    "    history : list of <NEEDS IMPLEMENTING>\n",
    "        Snapshots of the model state at each time step.\n",
    "    samples_seen : pandas.DataFrame\n",
    "        All of the molecules seen before. Includes initial training set.\n",
    "    smiles_seen : list of str\n",
    "        SMILES strings of the molecules seen before.\n",
    "    cost : int or float\n",
    "        Total monetary cost incurred at the current time.\n",
    "    number_molecules : int\n",
    "        Total number of molecules purchased at the current time.\n",
    "    time : int\n",
    "        Total number of days spent up to the current time.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, N, M, ensemble_size=3, epochs=1, molecule_cost=200,\n",
    "                 target_bounds={\"bace\":(4, math.inf), \"esol\":(-5, math.inf), \"logD\":(-0.4, 5.6)} ):\n",
    "        self.N = N #initial samples\n",
    "        self.M = M #batch size\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.epochs = epochs\n",
    "        self.molecule_cost = molecule_cost\n",
    "        self.target_bounds = target_bounds\n",
    "        \n",
    "        self.ensemble = {i:dc.models.GraphConvModel(n_tasks=3, mode='regression', batch_size=20, random_seed=i) \n",
    "                         for i in range(self.ensemble_size)} #map each model to its seed\n",
    "        self.history = [] #save snapshot of model, on disk\n",
    "        self.samples_seen = None\n",
    "        self.smiles_seen = []\n",
    "        self.cost = 0\n",
    "        self.number_molecules = 0\n",
    "        self.time = 0 #days\n",
    "        \n",
    "        \n",
    "    def train_model(self, model, dataset):\n",
    "        \"\"\"Helper function to train a given ensemble model on a given dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model : Keras model (generally deepchem.GraphConvModel)\n",
    "            Model to be trained.\n",
    "        dataset : pandas.DataFrame\n",
    "            Dataset to train on. Must include \"SMILES\", \"bace\", \"esol\", and \"logD\" headers.\n",
    "            \n",
    "        \"\"\"\n",
    "        #convert DataFrame to CSV and read in as deepchem.Dataset via deepchem.CSVLoader\n",
    "        dataset_temp_file = \"./temp/training_dataset.csv\"\n",
    "        dataset.to_csv(dataset_temp_file)\n",
    "\n",
    "        featurizer = dc.feat.ConvMolFeaturizer()\n",
    "        loader = dc.data.CSVLoader(tasks=[\"bace\", \"esol\", \"logD\"], smiles_field=\"SMILES\", featurizer=featurizer)\n",
    "\n",
    "        dataset_feat = loader.featurize(dataset_temp_file)\n",
    "\n",
    "        transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "        dataset_feat = transformer.transform(dataset_feat)\n",
    "\n",
    "        model.fit(dataset_feat, nb_epoch=1, deterministic=True, restore=False)\n",
    "    \n",
    "    \n",
    "    def train_ensemble(self, dataset):\n",
    "        \"\"\"Helper function to train model ensemble.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : pandas.Dataset\n",
    "            Dataset on which to train models. Must include \"SMILES\", \"bace\", \"esol\", and \"logD\" headers.\n",
    "        \n",
    "        \"\"\"\n",
    "        for model in self.ensemble.values():\n",
    "            self.train_model(model, dataset)\n",
    "\n",
    "    \n",
    "    def initial_training(self, verbose=False):\n",
    "        \"\"\"Train model ensemble for the first time on self.N samples randomly chosen from the 2500 lowest bace affinity-scored \n",
    "        molecules.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        verbose : bool\n",
    "            Whether to print progress updates.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        If self.N > 2500, ensemble will be trained on 2500 samples.\n",
    "        Records first history object.\n",
    "        \n",
    "        \"\"\"\n",
    "        idx_range = self.N if self.N < low_bace_dataset.shape[0] else low_bace_dataset.shape[0]\n",
    "        rand_indices = random.sample(range(low_bace_dataset.shape[0]), k=idx_range) #select random row indices\n",
    "        \n",
    "        init_ensemble_dataset = pd.DataFrame()\n",
    "        for idx in rand_indices:\n",
    "            init_ensemble_dataset = init_ensemble_dataset.append( low_bace_dataset.iloc[idx], ignore_index=True )\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Training set selected.\")\n",
    "            \n",
    "        self.samples_seen = init_ensemble_dataset ### collect the examples seen during initial training\n",
    "        self.smiles_seen = init_ensemble_dataset[\"SMILES\"].tolist()\n",
    "        \n",
    "        #cost/time to initially train? free initial knowledge?\n",
    "        self.cost += 200 * len(init_ensemble_dataset)\n",
    "        self.number_molecules += len(init_ensemble_dataset)\n",
    "        self.time = 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Training ensemble...\")\n",
    "            \n",
    "        self.train_ensemble(init_ensemble_dataset) #train ensemble on initial dataset\n",
    "        \n",
    "        self.record_history()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Ensemble trained.\")\n",
    "        \n",
    "        \n",
    "    ###get_component_score, ket/list(keys)\n",
    "    def get_component_score(self, arr, keys):\n",
    "        \"\"\"Helper function to get the scaled \"goodness\" of the input scores.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        array : numpy.array\n",
    "             Array with bace, esol, and logD scores.\n",
    "        keys : collection of strings from {\"bace\", \"esol\", \"logD\"}\n",
    "            Which scores to incorporate into the overall goodness.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array\n",
    "            Sum of component scores.\n",
    "        \n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        if \"bace\" in keys:\n",
    "            #higher bace => higher score\n",
    "            bace = arr[:,0]\n",
    "            bace_range = self.target_bounds[\"bace\"]\n",
    "            scores.append( np.where(bace < bace_range[0], 0.2*bace-0.8, 0.05*bace-0.2) )\n",
    "            #dec penalty when score>low end of range\n",
    "        \n",
    "        if \"esol\" in keys:\n",
    "            esol = arr[:,1]\n",
    "            esol_range = self.target_bounds[\"esol\"]\n",
    "            scores.append( np.where(esol < esol_range[0], esol - np.absolute(esol-esol_range[1])**2, esol) )\n",
    "        \n",
    "        if \"logD\" in keys:\n",
    "            #logD within range is not penalized\n",
    "            logD = arr[:,2]\n",
    "            logD_range = self.target_bounds[\"logD\"]\n",
    "            #handle lower end of range\n",
    "            int_arr = np.where(logD < logD_range[0], logD - np.absolute(logD-logD_range[0]), logD)\n",
    "            #handle upper end of range\n",
    "            scores.append(np.where(int_arr > logD_range[1], int_arr - np.absolute(int_arr-logD_range[1]), int_arr) )\n",
    "\n",
    "        return sum(scores)\n",
    "        \n",
    "    \n",
    "    def score_and_select_top(self):\n",
    "        \"\"\"Scores all molecules and selects the top M for \"purchase\".\n",
    "        \n",
    "        \"\"\"\n",
    "        featurizer = dc.feat.ConvMolFeaturizer()\n",
    "        loader = dc.data.CSVLoader(tasks=[\"bace\", \"esol\", \"logD\"], smiles_field=\"SMILES\", featurizer=featurizer)\n",
    "        dataset_feat = loader.featurize(dataset_file) #featurize the molecules from the ground truth dataset\n",
    "        transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "        dataset_feat = transformer.transform(dataset_feat)\n",
    "        \n",
    "        predicted = np.zeros( (len(dataset_feat),3) )\n",
    "        for model in self.ensemble.values():\n",
    "            predicted += model.predict(dataset_feat)\n",
    "        predicted /= len(self.ensemble) #take the average of model predictions\n",
    "        \n",
    "        #copy SMILES and assign/calculate scores\n",
    "        results_df = pd.DataFrame()\n",
    "        results_df[\"SMILES\"] = ground_truth_dataset[\"SMILES\"]   \n",
    "\n",
    "        goodness = self.get_component_score(predicted, [\"bace\", \"esol\", \"logD\"])\n",
    "        results_df[\"bace\"] = predicted[:,0]\n",
    "        results_df[\"esol\"] = predicted[:,1]\n",
    "        results_df[\"logD\"] = predicted[:,2]\n",
    "        results_df[\"goodness\"] = goodness\n",
    "        \n",
    "        unseen_rows = results_df.loc[~results_df['SMILES'].isin(self.smiles_seen)] #remove examples previously seen\n",
    "        unseen_rows = unseen_rows.sort_values(by=\"goodness\", ascending=False) #sort with highest goodness at top\n",
    "        \n",
    "        subset = unseen_rows[:self.M] if len(unseen_rows) > self.M else unseen_rows #select up to self.M samples\n",
    "        \n",
    "        self.samples_seen = pd.concat([self.samples_seen,subset], sort=False)\n",
    "        self.smiles_seen = self.samples_seen[\"SMILES\"].tolist()\n",
    "        #self.samples_seen = self.samples_seen.append(subset, sort=False) # ignore_index=True?\n",
    "        \n",
    "        self.cost += 200 * len(subset)\n",
    "        self.number_molecules += len(subset)\n",
    "            \n",
    "        self.time += 28 #4 weeks to buy and experiment\n",
    "        \n",
    "    \n",
    "    def record_history(self):\n",
    "        \"\"\"Stores model costs and experience for later analysis.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        Does not save self.history attribute, in order to avoid redundantly storing the data in it.\n",
    "        Only saves attributes that change in each time step.\n",
    "        \n",
    "        \"\"\"\n",
    "        hist = {}\n",
    "        hist[\"samples_seen\"] = self.samples_seen\n",
    "        hist[\"smiles_seen\"] = self.smiles_seen\n",
    "        hist[\"cost\"] = self.cost\n",
    "        hist[\"number_molecules\"] = self.number_molecules\n",
    "        hist[\"time\"] = self.time\n",
    "        self.history.append(hist)\n",
    "     \n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Simple wrapper to automate calls to select molecules and update models. \n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        Must be preceded by initial training of model ensemble.\n",
    "        \n",
    "        \"\"\"        \n",
    "        while len(self.samples_seen) < len(ground_truth_dataset): #replace with top bace score to exit -> 4.87(approx 95th %ile)\n",
    "            candidates = self.samples_seen.loc[self.samples_seen['bace'] >= top_5_percent_bace_cutoff] #find mols w/ high bace\n",
    "            \n",
    "            esol_lower_bound = self.target_bounds[\"esol\"][0]\n",
    "            candidates = candidates.loc[candidates['esol'] >= esol_lower_bound] #filter the insoluble mols\n",
    "            \n",
    "            logD_range = self.target_bounds[\"logD\"]\n",
    "            candidates = candidates.loc[( candidates['logD'] >= logD_range[0] ) \n",
    "                                                 & ( candidates['logD'] <= logD_range[1] )] #filter for logD in range\n",
    "              \n",
    "            if len(candidates) > 0:\n",
    "                print(\"Molecule within bounds and 95th percentile bace affinity found.\")\n",
    "                return candidates\n",
    "                \n",
    "            self.score_and_select_top()\n",
    "            self.record_history()\n",
    "            self.train_ensemble(self.samples_seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1: load ground truth models and ensemble\n",
    "\n",
    "Step 2: train ensemble on N random data points (including ground truth values)\n",
    "\n",
    "Step 3: score all of the 10K molecules using the ensemble\n",
    "\n",
    "Step 4: take (\"buy\") the top M, and \"assess them experimentally\" (get their ground truth values)\n",
    "\n",
    "Step 5: add those samples to the training/seen set\n",
    "\n",
    "Step 6: retrain the ensemble\n",
    "\n",
    "Step 7: repeat (make 2-6 repeatable)\n",
    "\n",
    "Step 8: add some loops over N and M to generate plots of Hx vs N,M\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.142 s\n",
      "TIMING: dataset construction took 0.276 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.128 s\n",
      "Loading dataset from disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darne\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending global_step 5: Average loss 1.15469\n",
      "TIMING: model fitting took 36.656 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 15.056 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.762 s\n",
      "TIMING: dataset construction took 22.810 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 9.095 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.283 s\n",
      "TIMING: dataset construction took 0.445 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.222 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 15: Average loss 1.37102\n",
      "TIMING: model fitting took 6.800 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 14.816 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.744 s\n",
      "TIMING: dataset construction took 22.136 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.562 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.661 s\n",
      "TIMING: dataset construction took 0.847 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.568 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 30: Average loss 0.943086\n",
      "TIMING: model fitting took 8.416 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 17.784 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.913 s\n",
      "TIMING: dataset construction took 27.201 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.090 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.816 s\n",
      "TIMING: dataset construction took 1.062 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.672 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 50: Average loss 0.985435\n",
      "TIMING: model fitting took 10.194 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 19.177 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.893 s\n",
      "TIMING: dataset construction took 28.624 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.865 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 2.493 s\n",
      "TIMING: dataset construction took 2.817 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.565 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 74: Average loss 1.11081\n",
      "TIMING: model fitting took 10.337 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 18.068 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.046 s\n",
      "TIMING: dataset construction took 26.920 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.632 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.989 s\n",
      "TIMING: dataset construction took 1.383 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.630 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 103: Average loss 1.09157\n",
      "WARNING:tensorflow:From C:\\Users\\darne\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py:624: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "TIMING: model fitting took 11.120 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 18.278 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.851 s\n",
      "TIMING: dataset construction took 27.998 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 9.498 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 1.056 s\n",
      "TIMING: dataset construction took 1.413 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.573 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 137: Average loss 1.13238\n",
      "TIMING: model fitting took 10.904 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 17.408 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.875 s\n",
      "TIMING: dataset construction took 25.014 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.427 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 1.608 s\n",
      "TIMING: dataset construction took 2.022 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.016 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 176: Average loss 1.16817\n",
      "TIMING: model fitting took 13.823 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 16.765 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.093 s\n",
      "TIMING: dataset construction took 25.370 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 10.525 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 1.851 s\n",
      "TIMING: dataset construction took 2.389 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.877 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 220: Average loss 1.1813\n",
      "TIMING: model fitting took 13.230 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 17.909 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.111 s\n",
      "TIMING: dataset construction took 26.245 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.893 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 1.819 s\n",
      "TIMING: dataset construction took 2.265 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.876 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 268: Average loss 1.20614\n",
      "TIMING: model fitting took 13.833 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 15.781 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.449 s\n",
      "TIMING: dataset construction took 25.215 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 10.380 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 1.804 s\n",
      "TIMING: dataset construction took 2.303 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.956 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 321: Average loss 1.43397\n",
      "TIMING: model fitting took 15.058 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 18.980 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 4.502 s\n",
      "TIMING: dataset construction took 28.975 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 9.969 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 2.225 s\n",
      "TIMING: dataset construction took 2.832 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.039 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 379: Average loss 1.52624\n",
      "TIMING: model fitting took 16.557 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 16.698 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.688 s\n",
      "TIMING: dataset construction took 24.472 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.462 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 1.997 s\n",
      "TIMING: dataset construction took 2.523 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.034 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 442: Average loss 1.5241\n",
      "TIMING: model fitting took 15.594 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 14.939 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.744 s\n",
      "TIMING: dataset construction took 23.078 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.147 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 2.189 s\n",
      "TIMING: dataset construction took 3.463 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.335 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 510: Average loss 1.29245\n",
      "TIMING: model fitting took 17.136 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 17.460 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.905 s\n",
      "TIMING: dataset construction took 26.365 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 12.724 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 2.709 s\n",
      "TIMING: dataset construction took 3.358 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.264 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 582: Average loss 1.07379\n",
      "TIMING: model fitting took 18.986 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 15.476 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.915 s\n",
      "TIMING: dataset construction took 23.249 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.578 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 2.477 s\n",
      "TIMING: dataset construction took 3.218 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.462 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 659: Average loss 1.15674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING: model fitting took 18.755 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 16.000 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.814 s\n",
      "TIMING: dataset construction took 24.617 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.602 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 3.464 s\n",
      "TIMING: dataset construction took 4.250 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.593 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 741: Average loss 1.19418\n",
      "TIMING: model fitting took 20.474 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 16.637 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.919 s\n",
      "TIMING: dataset construction took 24.463 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.938 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 3.789 s\n",
      "TIMING: dataset construction took 4.575 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.485 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 828: Average loss 1.23397\n",
      "TIMING: model fitting took 20.234 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 16.770 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.306 s\n",
      "TIMING: dataset construction took 25.752 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 10.742 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 2.991 s\n",
      "TIMING: dataset construction took 3.777 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.585 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 920: Average loss 1.22399\n",
      "TIMING: model fitting took 25.404 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 18.324 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.890 s\n",
      "TIMING: dataset construction took 26.072 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 10.101 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 3.103 s\n",
      "TIMING: dataset construction took 3.950 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.735 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 999: Average loss 1.49045\n",
      "Ending global_step 1016: Average loss 0.398768\n",
      "TIMING: model fitting took 22.202 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 15.049 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.817 s\n",
      "TIMING: dataset construction took 22.586 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 8.801 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 3.394 s\n",
      "TIMING: dataset construction took 4.572 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.757 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 1117: Average loss 1.30502\n",
      "TIMING: model fitting took 25.347 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 16.734 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.856 s\n",
      "TIMING: dataset construction took 25.361 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 11.066 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 3.311 s\n",
      "TIMING: dataset construction took 4.203 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.838 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 1223: Average loss 1.11148\n",
      "TIMING: model fitting took 24.005 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 14.560 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.067 s\n",
      "TIMING: dataset construction took 22.960 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 10.402 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 3.857 s\n",
      "TIMING: dataset construction took 4.849 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.626 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 1334: Average loss 1.22497\n",
      "TIMING: model fitting took 29.734 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 17.847 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.823 s\n",
      "TIMING: dataset construction took 25.548 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 11.976 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 4.768 s\n",
      "TIMING: dataset construction took 5.854 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.009 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 1450: Average loss 1.26441\n",
      "TIMING: model fitting took 27.815 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 15.264 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.742 s\n",
      "TIMING: dataset construction took 22.821 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 9.356 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 5.104 s\n",
      "TIMING: dataset construction took 6.314 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 3.128 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 1570: Average loss 1.28125\n",
      "TIMING: model fitting took 32.812 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 18.084 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.829 s\n",
      "TIMING: dataset construction took 25.778 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 10.181 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 4.201 s\n",
      "TIMING: dataset construction took 5.267 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.285 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 1695: Average loss 1.21456\n",
      "TIMING: model fitting took 31.530 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 21.990 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.472 s\n",
      "TIMING: dataset construction took 30.941 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 10.156 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 6.111 s\n",
      "TIMING: dataset construction took 7.650 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.544 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 1825: Average loss 1.1713\n",
      "TIMING: model fitting took 32.871 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 17.474 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.855 s\n",
      "TIMING: dataset construction took 25.532 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 9.036 s\n",
      "Loading dataset from disk.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-aebfc570b7bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitial_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mtop_mols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-92-b23baa7198da>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_and_select_top\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_ensemble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples_seen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-92-b23baa7198da>\u001b[0m in \u001b[0;36mscore_and_select_top\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_feat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m             \u001b[0mpredicted\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_feat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#take the average of model predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\deepchem\\models\\keras_model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, dataset, transformers, outputs)\u001b[0m\n\u001b[0;32m    669\u001b[0m     generator = self.default_generator(\n\u001b[0;32m    670\u001b[0m         dataset, mode='predict', pad_batches=False)\n\u001b[1;32m--> 671\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpredict_uncertainty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\deepchem\\models\\keras_model.py\u001b[0m in \u001b[0;36mpredict_on_generator\u001b[1;34m(self, generator, transformers, outputs)\u001b[0m\n\u001b[0;32m    592\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0mproduces\u001b[0m \u001b[0mmultiple\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m     \"\"\"\n\u001b[1;32m--> 594\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\deepchem\\models\\keras_model.py\u001b[0m in \u001b[0;36m_predict\u001b[1;34m(self, generator, transformers, outputs, uncertainty)\u001b[0m\n\u001b[0;32m    532\u001b[0m           \u001b[0mfetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_placeholders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m         \u001b[0moutput_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m       \u001b[1;31m# Apply tranformers and record results.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#N = [96, 384, 1536] #initial train set size\n",
    "N = [96]\n",
    "#M = [96, 384, 1536] #batch size -> 96 wells, multiples\n",
    "M = [96]\n",
    "\n",
    "models = []\n",
    "\n",
    "for n in N:\n",
    "    for m in M:\n",
    "        e = Experimenter(n, m, ensemble_size=1)\n",
    "        models.append(e)\n",
    "        e.initial_training()\n",
    "        top_mols = e.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for model in models:\n",
    "    costs = []\n",
    "    times = []\n",
    "    top_bace_scores = []\n",
    "    for hx in model.history:\n",
    "        costs.append(hx[\"cost\"])\n",
    "        times.append(hx[\"time\"])\n",
    "        top_bace_scores.append(hx[\"samples_seen\"][\"bace\"].max())\n",
    "\n",
    "        plt.plot(times,costs,'r--',label=\"cost\")\n",
    "        plt.plot(times,top_bace_scores,'bs',label=\"bace\")\n",
    "        plt.xlabel('Time')\n",
    "        plt.title(r'Time vs Cost and top BACE score')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
