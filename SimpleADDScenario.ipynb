{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of dataset: ['Unnamed: 0' 'SMILES' 'esol' 'logD' 'bace']\n",
      "Number of examples in dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "###initialize imports and dataset\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import deepchem as dc\n",
    "from deepchem.utils.save import load_from_disk\n",
    "from deepchem.data import data_loader\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "dataset_file = \"./enamineSubset10KGroundTruth.csv\"\n",
    "ground_truth_dataset = load_from_disk(dataset_file) #pandas Dataframe\n",
    "\n",
    "low_bace_dataset = ground_truth_dataset.copy(deep=True).sort_values(by=\"bace\")[:2500] #take 2500 worst binders as starting set candidates\n",
    "\n",
    "print(\"Columns of dataset: %s\" % str(ground_truth_dataset.columns.values))\n",
    "print(\"Number of examples in dataset: %s\" % str(ground_truth_dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###initialize ground truth models and methods to access them\n",
    "\n",
    "def load_oracle_models():\n",
    "    bace_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/bace\")\n",
    "    esol_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/esol\")\n",
    "    logD_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/logD\")\n",
    "    bace_model.restore()\n",
    "    esol_model.restore()\n",
    "    logD_model.restore()\n",
    "    oracle = {\"bace\":bace_model, \"esol\":esol_model, \"logD\":logD_model} #get each model via the named property\n",
    "    return oracle\n",
    "    \n",
    "def query_oracle(smiles):\n",
    "    ### use when evaluating on the fly\n",
    "    raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model = dc.models.GraphConvModel(n_tasks=3, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/test_model\")\n",
    "\n",
    "from deepchem.data.data_loader import featurize_smiles_df\n",
    "\n",
    "def train_model(model, dataset):\n",
    "    #take in dataset as pandas Dataframe and convert to dc Dataset via pd to_csv and dc CSVLoader\n",
    "    dataset_temp_file = \"./temp/training_dataset.csv\"\n",
    "    dataset.to_csv(dataset_temp_file)\n",
    "    \n",
    "    featurizer = dc.feat.ConvMolFeaturizer()\n",
    "    loader = dc.data.CSVLoader(tasks=[\"bace\", \"logD\", \"esol\"], smiles_field=\"SMILES\", featurizer=featurizer)\n",
    "    \n",
    "    dataset_feat = loader.featurize(dataset_temp_file)\n",
    "\n",
    "    transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "    dataset_feat = transformer.transform(dataset_feat)\n",
    "    \n",
    "    model.fit(dataset_feat, nb_epoch=1, deterministic=True)\n",
    "    #results = test_model.predict(dataset_feat)\n",
    "    \n",
    "#train_model(test_model, low_bace_dataset[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "###define Abstract Data Type to hold search information, including ensemble\n",
    "\n",
    "class Experimenter():\n",
    "    def __init__(self, N, M):\n",
    "        self.N = N #how many random samples to initially train on\n",
    "        self.M = M #the size of batch to purchase\n",
    "        self.ensemble = {i:dc.models.GraphConvModel(n_tasks=3, mode='regression', batch_size=50, random_seed=i) for i in range(100)} #map each model to its seed\n",
    "        self.history = []\n",
    "        self.samples_seen = None\n",
    "              \n",
    "        self.bace_range = (4, math.inf)\n",
    "        self.esol_range = (-5, math.inf)\n",
    "        self.logD_range = (-0.4, 5.6)\n",
    "        \n",
    "        ###train the ensemble on a subset that binds bace poorly\n",
    "        if N > dataset.shape[0]:\n",
    "            N = dataset.shape[0]\n",
    "        rand_indices = random.sample(range(dataset.shape[0]), k=N) #select random row indices\n",
    "\n",
    "        init_ensemble_dataset = pd.DataFrame()\n",
    "        for idx in rand_indices:\n",
    "            init_ensemble_dataset = init_ensemble_dataset.append( low_bace_dataset.iloc[idx], ignore_index=True )\n",
    "        \n",
    "        self.samples_seen = init_ensemble_dataset ### collect the examples seen during initial training\n",
    "        \n",
    "        train_ensemble(init_ensemble_dataset) #train ensemble on initialization\n",
    "\n",
    "        \n",
    "    def get_bace_score(x):\n",
    "        #higher bace => higher score\n",
    "        return np.where(x < self.bace_range[0], 0.2*x-0.8, 0.05*x-0.2) #decrease penalty once score > lower end of range\n",
    "\n",
    "    def get_logD_score(x):\n",
    "        #logD within range is not penalized, \n",
    "        x = np.where(x < self.logD_range[0], x - np.absolute(x-self.logD_range[0]), x) #handle lower end of range\n",
    "        return np.where(x > self.logD_range[1], x - np.absolute(x-self.logD_range[1]), x) #handle upper end of range\n",
    "   \n",
    "    def get_esol_score(x):\n",
    "        return np.where(x < self.esol_range[0], x - np.absolute(x-self.logD_range[1])**2, x)\n",
    "        \n",
    "    def get_goodness_score(bace, logD, esol):\n",
    "        return get_bace_score(bace) + get_logD_score(logD) + get_esol_score(esol)\n",
    "    \n",
    "    \n",
    "    def train_ensemble(self, dataset):\n",
    "        for model in self.ensemble:\n",
    "            train_model(model, dataset)\n",
    "    \n",
    "    \n",
    "    def score_and_select_top():\n",
    "        featurizer = dc.feat.ConvMolFeaturizer()\n",
    "        loader = dc.data.CSVLoader(tasks=[\"bace\", \"logD\", \"esol\"], smiles_field=\"SMILES\", featurizer=featurizer)\n",
    "        dataset = loader.featurize()\n",
    "        \n",
    "        predicted = np.zeroes( (len(ground_truth_dataset),3) )\n",
    "        for model in self.ensemble.values():\n",
    "            predicted += model.predict(ground_truth_dataset)\n",
    "                              \n",
    "        predicted /= len(self.ensemble) #take the average of the model predictions\n",
    "            \n",
    "        results_df = pd.DataFrame()\n",
    "        results_df[\"SMILES\"] = ground_truth_dataset[\"SMILES\"]\n",
    "        \n",
    "        bace = results[:,0]\n",
    "        logD = results[:,1]\n",
    "        esol = results[:,2]\n",
    "        goodness = self.get_goodness_score(bace, logD, esol)\n",
    "        \n",
    "        results_df[\"bace\"] = bace\n",
    "        results_df[\"logD\"] = logD\n",
    "        results_df[\"esol\"] = esol\n",
    "        results_df[\"goodness\"] = goodness\n",
    "        print(results_df)\n",
    "        \n",
    "        results_df.sort_values(by=\"goodness\")\n",
    "        return results_df[-self.M:]\n",
    "     \n",
    "        return predicted[-self.M:,:] #return molecules with M highest goodness scores\n",
    "    \n",
    "    \n",
    "    def run():\n",
    "        raise NotImplementedError\n",
    "        while not STOP_CONDITION:\n",
    "            score_and_select_top()\n",
    "            #record history\n",
    "            #update dataset\n",
    "            train_ensemble(self.samples_seen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###define property preference fxns and ranges, cost fxns\n",
    "\n",
    "def get_bace_score():\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def get_esol_score():\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def get_logD_score():\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def get_goodness_score():\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1: load ground truth models and ensemble\n",
    "\n",
    "Step 2: train ensemble on N random data points (including ground truth values)\n",
    "\n",
    "Step 3: score all of the 10K molecules using the ensemble\n",
    "\n",
    "Step 4: take (\"buy\") the top M, and \"assess them experimentally\" (get their ground truth values)\n",
    "\n",
    "Step 5: add those samples to the training/seen set\n",
    "\n",
    "Step 6: retrain the ensemble\n",
    "\n",
    "Step 7: repeat (make 2-6 repeatable)\n",
    "\n",
    "Step 8: add some loops over N and M to generate plots of Hx vs N,M\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in N_set:\n",
    "    for m in M_set:\n",
    "        E = Experimenter(n, m)\n",
    "        e.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
