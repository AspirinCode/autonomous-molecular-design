{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of dataset: ['Unnamed: 0' 'SMILES' 'esol' 'logD' 'bace']\n",
      "Number of examples in dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "###initialize imports and dataset\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import deepchem as dc\n",
    "from deepchem.utils.save import load_from_disk\n",
    "from deepchem.data import data_loader\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "dataset_file = \"./enamineSubset10KGroundTruth.csv\"\n",
    "ground_truth_dataset = load_from_disk(dataset_file) #pandas Dataframe\n",
    "\n",
    "low_bace_dataset = ground_truth_dataset.copy(deep=True).sort_values(by=\"bace\")[:2500] #take 2500 worst binders as starting set candidates\n",
    "\n",
    "print(\"Columns of dataset: %s\" % str(ground_truth_dataset.columns.values))\n",
    "print(\"Number of examples in dataset: %s\" % str(ground_truth_dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###initialize ground truth models and methods to access them\n",
    "\n",
    "def load_oracle_models():\n",
    "    bace_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/bace\")\n",
    "    esol_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/esol\")\n",
    "    logD_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/logD\")\n",
    "    bace_model.restore()\n",
    "    esol_model.restore()\n",
    "    logD_model.restore()\n",
    "    oracle = {\"bace\":bace_model, \"esol\":esol_model, \"logD\":logD_model} #get each model via the named property\n",
    "    return oracle\n",
    "    \n",
    "def query_oracle(smiles):\n",
    "    ### use when evaluating on the fly\n",
    "    raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.022 s\n",
      "TIMING: dataset construction took 0.095 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.049 s\n",
      "Loading dataset from disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darne\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending global_step 1: Average loss 0.645826\n",
      "TIMING: model fitting took 38.598 s\n",
      "[[ 1.0481931  -0.80038565  0.11046308]\n",
      " [ 1.4108983  -0.89190555 -0.3014657 ]\n",
      " [ 1.2603654  -1.2325282  -0.25898513]\n",
      " [ 1.1178118  -1.1508771  -0.16457354]\n",
      " [ 1.4231299  -0.7285227   0.07284688]\n",
      " [ 0.9994026  -0.58478886 -0.14232585]\n",
      " [ 1.1015353  -0.8567252   0.08157533]\n",
      " [ 0.844594   -0.6443539   0.19544438]\n",
      " [ 1.5114546  -1.0783786  -0.1482876 ]\n",
      " [ 1.0526407  -1.2137191  -0.13992053]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[ 0.844594   -0.6443539   0.19544438]\n",
      " [ 0.9994026  -0.58478886 -0.14232585]\n",
      " [ 1.0481931  -0.80038565  0.11046308]\n",
      " [ 1.0526407  -1.2137191  -0.13992053]\n",
      " [ 1.1015353  -0.8567252   0.08157533]\n",
      " [ 1.1178118  -1.1508771  -0.16457354]\n",
      " [ 1.2603654  -1.2325282  -0.25898513]\n",
      " [ 1.4108983  -0.89190555 -0.3014657 ]\n",
      " [ 1.4231299  -0.7285227   0.07284688]\n",
      " [ 1.5114546  -1.0783786  -0.1482876 ]]\n"
     ]
    }
   ],
   "source": [
    "test_model = dc.models.GraphConvModel(n_tasks=3, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/test_model\")\n",
    "\n",
    "from deepchem.data.data_loader import featurize_smiles_df\n",
    "\n",
    "def train_model(model, dataset):\n",
    "    dataset_temp_file = \"./temp/training_dataset.csv\"\n",
    "    dataset.to_csv(dataset_temp_file)\n",
    "    \n",
    "    featurizer = dc.feat.ConvMolFeaturizer()\n",
    "    loader = dc.data.CSVLoader(tasks=[\"bace\", \"logD\", \"esol\"], smiles_field=\"SMILES\", featurizer=featurizer)\n",
    "    \n",
    "    dataset = loader.featurize(dataset_temp_file)\n",
    "    #dataset = dc.data.data_loader.featurize_smiles_df(dataset, featurizer, \"SMILES\", verbose=False)\n",
    "    ###this needs to be converted to a dc Dataset (DiskDataset with tempdir, NumpyDataset???)\n",
    "\n",
    "    transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset)\n",
    "    dataset = transformer.transform(dataset)\n",
    "    \n",
    "    model.fit(dataset, nb_epoch=1, deterministic=True)\n",
    "    \n",
    "train_model(test_model, low_bace_dataset[-10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "###define Abstract Data Type to hold search information, including ensemble\n",
    "\n",
    "class Experimenter():\n",
    "    def __init__(self, N, M):\n",
    "        self.N = N #how many random samples to initially train on\n",
    "        self.M = M #the size of batch to purchase\n",
    "        self.ensemble = {i:dc.models.GraphConvModel(n_tasks=3, mode='regression', batch_size=50, random_seed=i) for i in range(100)} #map each model to its seed\n",
    "        self.history = []\n",
    "        \n",
    "        ###train the ensemble on a subset that binds bace poorly\n",
    "        if N > dataset.shape[0]:\n",
    "            N = dataset.shape[0]\n",
    "        rand_indices = random.sample(range(dataset.shape[0]), k=N) #select random row indices\n",
    "\n",
    "        init_ensemble_dataset = pd.DataFrame()\n",
    "        for idx in rand_indices:\n",
    "            init_ensemble_dataset = init_ensemble_dataset.append( low_bace_dataset.iloc[idx], ignore_index=True )\n",
    "        \n",
    "        self.samples_seen = init_ensemble_dataset ### collect the examples seen during initial training\n",
    "        \n",
    "        train_ensemble(init_ensemble_dataset) #train ensemble on initialization\n",
    "    \n",
    "    \n",
    "    def train_ensemble(self, dataset):\n",
    "        for model in self.ensemble:\n",
    "            train_model(model, dataset)\n",
    "    \n",
    "    \n",
    "    def score_and_select_top():\n",
    "        featurizer = dc.feat.ConvMolFeaturizer()\n",
    "        loader = dc.data.CSVLoader(tasks=[\"bace\", \"logD\", \"esol\"], smiles_field=\"SMILES\", featurizer=featurizer)\n",
    "        dataset = loader.featurize()\n",
    "        \n",
    "        predicted = np.zeroes( (len(dataset),3) )\n",
    "        for model in self.ensemble.values():\n",
    "            predicted += model.predict(ground_truth_dataset)\n",
    "                              \n",
    "        predicted /= len(self.ensemble) #take the average of the model predictions\n",
    "        \n",
    "        ### need some way to attach smiles labels to the scores -> DF, auto line up                      \n",
    "                              \n",
    "        ### ADD compute goodness scores, REPLACE sort to sort on goodness\n",
    "        predicted[predicted[:,0].argsort()] #sort by first column (bace affinity), highest at bottom\n",
    "\n",
    "        return predicted[-self.M:,:] #return molecules with M highest goodness scores\n",
    "    \n",
    "    \n",
    "    def run()\n",
    "        while not STOP_CONDITION:\n",
    "            score_and_select_top()\n",
    "            #record history\n",
    "            #update dataset\n",
    "            train_ensemble(self.samples_seen)\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###define property preference fxns and ranges, cost fxns\n",
    "\n",
    "bace_range = (4, math.inf)\n",
    "esol_range = (-5, math.inf)\n",
    "logD_range = (-0.4, 5.6)\n",
    "\n",
    "### all of these reference ground truth dataframe\n",
    "def get_bace_score():\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def get_esol_score():\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def get_logD_score():\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def get_goodness_score():\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1: load ground truth models and ensemble\n",
    "\n",
    "Step 2: train ensemble on N random data points (including ground truth values)\n",
    "\n",
    "Step 3: score all of the 10K molecules using the ensemble\n",
    "\n",
    "Step 4: take (\"buy\") the top M, and \"assess them experimentally\" (get their ground truth values)\n",
    "\n",
    "Step 5: add those samples to the training/seen set\n",
    "\n",
    "Step 6: retrain the ensemble\n",
    "\n",
    "Step 7: repeat (make 2-6 repeatable)\n",
    "\n",
    "Step 8: add some loops over N and M to generate plots of Hx vs N,M\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in N_set:\n",
    "    for m in M_set:\n",
    "        E = Experimenter(n, m)\n",
    "        e.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
