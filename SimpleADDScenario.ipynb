{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of dataset: ['Unnamed: 0' 'SMILES' 'esol' 'logD' 'bace']\n",
      "Number of examples in dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "###initialize imports and dataset\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import deepchem as dc\n",
    "from deepchem.utils.save import load_from_disk\n",
    "from deepchem.data import data_loader\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "dataset_file = \"./enamineSubset10KGroundTruth.csv\"\n",
    "ground_truth_dataset = load_from_disk(dataset_file) #pandas Dataframe\n",
    "\n",
    "low_bace_dataset = ground_truth_dataset.copy(deep=True).sort_values(by=\"bace\")[:2500] #take 2.5K worst binder potential starters\n",
    "\n",
    "print(\"Columns of dataset: %s\" % str(ground_truth_dataset.columns.values))\n",
    "print(\"Number of examples in dataset: %s\" % str(ground_truth_dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###initialize ground truth models and methods to access them\n",
    "\n",
    "def load_oracle_models():\n",
    "    \"\"\"Loads the pretrained ground truth models for evaluating molecules' properties on-the-fly.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    oracle : dict\n",
    "        A dictionary containing models mapped to their property keywords \\\"bace\\\", \\\"esol\\\", \\\"logD\\\".\n",
    "    \"\"\"\n",
    "    bace_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/bace\")\n",
    "    esol_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/esol\")\n",
    "    logD_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/logD\")\n",
    "    bace_model.restore()\n",
    "    esol_model.restore()\n",
    "    logD_model.restore()\n",
    "    oracle = {\"bace\":bace_model, \"esol\":esol_model, \"logD\":logD_model} #get each model via the named property\n",
    "    return oracle\n",
    "\n",
    "def query_oracle(dataset, oracle):\n",
    "    \"\"\"Evaluate molecules on-the-fly for their estimated bace, esol, and logD scores.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pandas.DataFrame\n",
    "        The input dataset; must includes a field with smiles strings under keyword \"SMILES\".\n",
    "    oracle : dictionary( dc.models.GraphConvModel )\n",
    "        The pretrained ground truth value prediction models.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    results : pandas.DataFrame\n",
    "        Copy of input dataset with newly estimated bace, esol, and logD scores under those headers. \n",
    "    \"\"\"\n",
    "    query_file = \"./temp/oracle_eval.csv\"\n",
    "    dataset.to_csv(query_file)\n",
    "    \n",
    "    results = dataset.copy(deep=True) #defensive copy of input dataframe \n",
    "    \n",
    "    featurizer = dc.feat.ConvMolFeaturizer()\n",
    "    for prop in (\"bace\", \"esol\", \"logD\"):\n",
    "        #retrieve appropriate model from oracle\n",
    "        model = oracle[prop]\n",
    "        \n",
    "        #load, featurize, and normalize input dataset\n",
    "        loader = dc.data.CSVLoader(tasks=[prop], smiles_field=\"SMILES\",featurizer=featurizer)\n",
    "        dataset_feat = loader.featurize(query_file)\n",
    "        transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "        dataset_feat = transformer.transform(dataset_feat)\n",
    "        \n",
    "        #predict and assign property results to keyword\n",
    "        predicted = model.predict(dataset_feat)\n",
    "        results[prop] = predicted\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###define Abstract Data Type to hold search information, including ensemble\n",
    "\n",
    "class Experimenter():\n",
    "    \"\"\"Class representing a research scientist/team going through the drug development process.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        Number of samples to initially train the experimenter ensemble on.\n",
    "    M : int\n",
    "        Number of molecules to purchase in each batch.\n",
    "    ensemble_size : int\n",
    "        Number of models in experimenter ensemble.\n",
    "    epochs : int\n",
    "        Number of epochs to train ensemble models for at each stage.\n",
    "    molecule_cost : int or float\n",
    "        Monetary cost of purchasing a single molecule.\n",
    "    target_bounds : dictionary of str:tuples(floats)\n",
    "        Desired range for each property.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    ensemble : dictionary of deepchem.models.GrachConvModel\n",
    "        Models representing the experimenter knowledge/predictions and uncertainty.\n",
    "    history : list of <NEEDS IMPLEMENTING>\n",
    "    samples_seen : pandas.DataFrame\n",
    "        All of the molecules seen before. Includes initial training set.\n",
    "    cost : int or float\n",
    "        Total monetary cost incurred at the current time.\n",
    "    number_molecules : int\n",
    "        Total number of molecules purchased at the current time.\n",
    "    time : int\n",
    "        Total number of days spent up to the current time.\n",
    "    \"\"\"\n",
    "    def __init__(self, N, M, ensemble_size=3, epochs=1, molecule_cost=200,\n",
    "                 target_bounds={\"bace\":(4, math.inf), \"esol\":(-5, math.inf), \"logD\":(-0.4, 5.6)} ):\n",
    "        self.N = N #how many random samples to initially train on\n",
    "        self.M = M #the size of batch to purchase\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.epochs = epochs\n",
    "        self.molecule_cost = molecule_cost\n",
    "        self.target_bounds = target_bounds\n",
    "        \n",
    "        self.ensemble = {i:dc.models.GraphConvModel(n_tasks=3, mode='regression', batch_size=20, random_seed=i) for i in range(3)} #map each model to its seed\n",
    "        self.history = [] #save snapshot of model, on disk\n",
    "        self.samples_seen = None\n",
    "        self.cost = 0\n",
    "        self.number_molecules = 0\n",
    "        self.time = 0 #days\n",
    "        \n",
    "        \n",
    "    def train_model(self, model, dataset):\n",
    "        #take in dataset as pandas Dataframe and convert to dc Dataset via pd to_csv and dc CSVLoader\n",
    "        dataset_temp_file = \"./temp/training_dataset.csv\"\n",
    "        dataset.to_csv(dataset_temp_file)\n",
    "\n",
    "        featurizer = dc.feat.ConvMolFeaturizer()\n",
    "        loader = dc.data.CSVLoader(tasks=[\"bace\", \"logD\", \"esol\"], smiles_field=\"SMILES\", featurizer=featurizer)\n",
    "\n",
    "        dataset_feat = loader.featurize(dataset_temp_file)\n",
    "\n",
    "        transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "        dataset_feat = transformer.transform(dataset_feat)\n",
    "\n",
    "        model.fit(dataset_feat, nb_epoch=1, deterministic=True, restore=False)\n",
    "    \n",
    "    def train_ensemble(self, dataset):\n",
    "        for model in self.ensemble.values():\n",
    "            train_model(model, dataset)\n",
    "\n",
    "    \n",
    "    def initial_training(self):\n",
    "        ###train the ensemble on a subset that binds bace poorly\n",
    "        if self.N > low_bace_dataset.shape[0]:\n",
    "            self.N = low_bace_dataset.shape[0]\n",
    "        rand_indices = random.sample(range(low_bace_dataset.shape[0]), k=self.N) #select random row indices\n",
    "        \n",
    "        print(\"Random initial training indices selected.\")\n",
    "\n",
    "        init_ensemble_dataset = pd.DataFrame()\n",
    "        for idx in rand_indices:\n",
    "            init_ensemble_dataset = init_ensemble_dataset.append( low_bace_dataset.iloc[idx], ignore_index=True )\n",
    "        \n",
    "        print(\"Initial training dataset selected.\")\n",
    "        \n",
    "        self.samples_seen = init_ensemble_dataset ### collect the examples seen during initial training\n",
    "        self.cost += 200 * len(init_ensemble_dataset)\n",
    "        self.number_molecules += len(init_ensemble_dataset)\n",
    "        self.time = 0 ### time to initially train? free initial knowledge?\n",
    "        \n",
    "        print(\"Training ensemble...\")\n",
    "        self.train_ensemble(init_ensemble_dataset) #train ensemble on initialization\n",
    "        print(\"Ensemble trained.\")\n",
    "        \n",
    "        \n",
    "    ###get_component_score, ket/list(keys),\n",
    "    def get_bace_score(self, x):\n",
    "        #higher bace => higher score\n",
    "        return np.where(x < self.bace_range[0], 0.2*x-0.8, 0.05*x-0.2) #decrease penalty once score > lower end of range\n",
    "\n",
    "    def get_logD_score(self, x):\n",
    "        #logD within range is not penalized, \n",
    "        x = np.where(x < self.logD_range[0], x - np.absolute(x-self.logD_range[0]), x) #handle lower end of range\n",
    "        return np.where(x > self.logD_range[1], x - np.absolute(x-self.logD_range[1]), x) #handle upper end of range\n",
    "   \n",
    "    def get_esol_score(self, x):\n",
    "        return np.where(x < self.esol_range[0], x - np.absolute(x-self.logD_range[1])**2, x)\n",
    "        \n",
    "    def get_goodness_score(self, bace, logD, esol):\n",
    "        return self.get_bace_score(bace) + self.get_logD_score(logD) + self.get_esol_score(esol)\n",
    "    \n",
    "    \n",
    "    def score_and_select_top(self):\n",
    "\n",
    "        featurizer = dc.feat.ConvMolFeaturizer()\n",
    "        loader = dc.data.CSVLoader(tasks=[\"bace\", \"logD\", \"esol\"], smiles_field=\"SMILES\", featurizer=featurizer)\n",
    "        dataset_feat = loader.featurize(dataset_file)\n",
    "        transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "        dataset_feat = transformer.transform(dataset_feat)\n",
    "        \n",
    "        predicted = np.zeros( (len(dataset_feat),3) )\n",
    "        for model in self.ensemble.values():\n",
    "            predicted += model.predict(dataset_feat)\n",
    "        predicted /= len(self.ensemble) #take the average of model predictions\n",
    "        \n",
    "        #copy SMILES and assign/calculate scores\n",
    "        results_df = pd.DataFrame()\n",
    "        results_df[\"SMILES\"] = ground_truth_dataset[\"SMILES\"]   \n",
    "        bace = predicted[:,0]\n",
    "        logD = predicted[:,1]\n",
    "        esol = predicted[:,2]\n",
    "        goodness = self.get_goodness_score(bace, logD, esol)\n",
    "        results_df[\"bace\"] = bace\n",
    "        results_df[\"logD\"] = logD\n",
    "        results_df[\"esol\"] = esol\n",
    "        results_df[\"goodness\"] = goodness\n",
    "        \n",
    "        seen_smiles = self.samples_seen[\"SMILES\"].tolist()\n",
    "        \n",
    "        unseen_rows = results_df.loc[~results_df['SMILES'].isin(seen_smiles)] #remove examples previously seen\n",
    "        unseen_rows.sort_values(by=\"goodness\", ascending=False) #sort with highest goodness at top\n",
    "        \n",
    "        if len(unseen_rows) > self.M:\n",
    "            subset = unseen_rows[:self.M]\n",
    "        else:\n",
    "            subset = unseen_rows\n",
    "        \n",
    "        self.samples_seen.append(subset, sort=False) # ignore_index=True?\n",
    "        self.cost += 200 * len(subset)\n",
    "        self.number_molecules += len(subset)\n",
    "            \n",
    "        self.time += 28 #4 weeks to buy and experiment\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        while len(self.samples_seen) < len(ground_truth_dataset): #replace with top bace score to exit\n",
    "            self.score_and_select_top()\n",
    "            #record history\n",
    "            self.train_ensemble(self.samples_seen)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStep 1: load ground truth models and ensemble\\n\\nStep 2: train ensemble on N random data points (including ground truth values)\\n\\nStep 3: score all of the 10K molecules using the ensemble\\n\\nStep 4: take (\"buy\") the top M, and \"assess them experimentally\" (get their ground truth values)\\n\\nStep 5: add those samples to the training/seen set\\n\\nStep 6: retrain the ensemble\\n\\nStep 7: repeat (make 2-6 repeatable)\\n\\nStep 8: add some loops over N and M to generate plots of Hx vs N,M\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 1: load ground truth models and ensemble\n",
    "\n",
    "Step 2: train ensemble on N random data points (including ground truth values)\n",
    "\n",
    "Step 3: score all of the 10K molecules using the ensemble\n",
    "\n",
    "Step 4: take (\"buy\") the top M, and \"assess them experimentally\" (get their ground truth values)\n",
    "\n",
    "Step 5: add those samples to the training/seen set\n",
    "\n",
    "Step 6: retrain the ensemble\n",
    "\n",
    "Step 7: repeat (make 2-6 repeatable)\n",
    "\n",
    "Step 8: add some loops over N and M to generate plots of Hx vs N,M\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [10] #initial train set size\n",
    "M = [10] #batch size -> 96 wells, multiples\n",
    "for n in N:\n",
    "    for m in M:\n",
    "        E = Experimenter(n, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bace': (4, inf), 'esol': (-5, inf), 'logD': (-0.4, 5.6)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E.target_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random initial training indices selected.\n",
      "Initial training dataset selected.\n",
      "Training ensemble...\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.086 s\n",
      "TIMING: dataset construction took 0.145 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.043 s\n",
      "Loading dataset from disk.\n",
      "WARNING:tensorflow:From C:\\Users\\darne\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:317: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From C:\\Users\\darne\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darne\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending global_step 1: Average loss 0.761683\n",
      "TIMING: model fitting took 12.007 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.020 s\n",
      "TIMING: dataset construction took 0.075 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.025 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 1: Average loss 0.771387\n",
      "TIMING: model fitting took 13.483 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.023 s\n",
      "TIMING: dataset construction took 0.086 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.036 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 1: Average loss 0.728813\n",
      "TIMING: model fitting took 14.577 s\n",
      "Ensemble trained.\n"
     ]
    }
   ],
   "source": [
    "E.initial_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 16.421 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.436 s\n",
      "TIMING: dataset construction took 25.362 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 6.795 s\n",
      "Loading dataset from disk.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Experimenter' object has no attribute 'bace_range'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-d7ab8524a771>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-245ad0226867>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples_seen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mground_truth_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#replace with top bace score to exit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_and_select_top\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m             \u001b[1;31m#record history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_ensemble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples_seen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-245ad0226867>\u001b[0m in \u001b[0;36mscore_and_select_top\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[0mlogD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mesol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mgoodness\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_goodness_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mesol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m         \u001b[0mresults_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"bace\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mresults_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"logD\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-245ad0226867>\u001b[0m in \u001b[0;36mget_goodness_score\u001b[1;34m(self, bace, logD, esol)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_goodness_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mesol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_bace_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbace\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_logD_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogD\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_esol_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mesol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-245ad0226867>\u001b[0m in \u001b[0;36mget_bace_score\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_bace_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;31m#higher bace => higher score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbace_range\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#decrease penalty once score > lower end of range\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_logD_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Experimenter' object has no attribute 'bace_range'"
     ]
    }
   ],
   "source": [
    "E.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
