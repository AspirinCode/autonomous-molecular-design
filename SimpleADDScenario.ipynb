{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff bace score for 95th percentile: 4.870083\n",
      "Columns of dataset: ['Index' 'SMILES' 'bace' 'esol' 'logD']\n",
      "Number of examples in dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "###initialize imports and dataset\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import deepchem as dc\n",
    "from deepchem.utils.save import load_from_disk\n",
    "from deepchem.data import data_loader\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "dataset_file = \"./enamineSubset10KGroundTruth.csv\"\n",
    "ground_truth_dataset = load_from_disk(dataset_file) #pandas Dataframe\n",
    "\n",
    "low_bace_dataset = ground_truth_dataset.sort_values(by=\"bace\")[:2500] #take 2.5K worst binder potential starters,shouldn't need copy\n",
    "\n",
    "top_5_percent_index = len(ground_truth_dataset) // 20\n",
    "top_5_percent_bace_cutoff = ground_truth_dataset.sort_values(by=\"bace\", ascending=True,)[\"bace\"][top_5_percent_index]\n",
    "\n",
    "print(\"Cutoff bace score for 95th percentile:\", top_5_percent_bace_cutoff)\n",
    "print(\"Columns of dataset: %s\" % str(ground_truth_dataset.columns.values))\n",
    "print(\"Number of examples in dataset: %s\" % str(ground_truth_dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###initialize ground truth models and methods to access them\n",
    "\n",
    "def load_oracle_models():\n",
    "    \"\"\"Loads the pretrained ground truth models for evaluating molecules' properties on-the-fly.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    oracle : dict\n",
    "        A dictionary containing models mapped to their property keywords: \"bace\", \"esol\", \"logD\".\n",
    "    \"\"\"\n",
    "    bace_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/bace\")\n",
    "    esol_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/esol\")\n",
    "    logD_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/logD\")\n",
    "    bace_model.restore()\n",
    "    esol_model.restore()\n",
    "    logD_model.restore()\n",
    "    oracle = {\"bace\":bace_model, \"esol\":esol_model, \"logD\":logD_model} #get each model via the named property\n",
    "    return oracle\n",
    "\n",
    "def query_oracle(dataset, oracle):\n",
    "    \"\"\"Evaluate molecules on-the-fly for their estimated bace, esol, and logD scores.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pandas.DataFrame\n",
    "        The input dataset; must includes a field with smiles strings under keyword \"SMILES\".\n",
    "    oracle : dictionary( dc.models.GraphConvModel )\n",
    "        The pretrained ground truth value prediction models.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    results : pandas.DataFrame\n",
    "        Copy of input dataset with newly estimated bace, esol, and logD scores under those headers. \n",
    "    \"\"\"\n",
    "    query_file = \"./temp/oracle_eval.csv\"\n",
    "    dataset.to_csv(query_file)\n",
    "    \n",
    "    results = dataset.copy(deep=True) #defensive copy of input dataframe \n",
    "    \n",
    "    featurizer = dc.feat.ConvMolFeaturizer()\n",
    "    for prop in (\"bace\", \"esol\", \"logD\"):\n",
    "        #retrieve appropriate model from oracle\n",
    "        model = oracle[prop]\n",
    "        \n",
    "        #load, featurize, and normalize input dataset\n",
    "        loader = dc.data.CSVLoader(tasks=[prop], smiles_field=\"SMILES\",featurizer=featurizer)\n",
    "        dataset_feat = loader.featurize(query_file)\n",
    "        transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "        dataset_feat = transformer.transform(dataset_feat)\n",
    "        \n",
    "        #predict and assign property results to keyword\n",
    "        predicted = model.predict(dataset_feat)\n",
    "        results[prop] = predicted\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "###define Abstract Data Type to hold search information, including ensemble\n",
    "\n",
    "class Experimenter():\n",
    "    \"\"\"Class representing a research scientist/team going through the drug development process.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        Number of samples to initially train the experimenter ensemble on.\n",
    "    M : int\n",
    "        Number of molecules to purchase in each batch.\n",
    "    ensemble_size : int, optional\n",
    "        Number of models in experimenter ensemble.\n",
    "    epochs : int, optional\n",
    "        Number of epochs to train ensemble models for at each stage.\n",
    "    molecule_cost : int or float, optional\n",
    "        Monetary cost of purchasing a single molecule.\n",
    "    target_bounds : dictionary of str:tuples(floats), optional\n",
    "        Desired range for each property.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    ensemble : dictionary of deepchem.models.GrachConvModel\n",
    "        Models representing the experimenter knowledge/predictions and uncertainty.\n",
    "    history : list of <NEEDS IMPLEMENTING>\n",
    "        Snapshots of the model state at each time step.\n",
    "    samples_seen : pandas.DataFrame\n",
    "        All of the molecules seen before. Includes initial training set.\n",
    "    smiles_seen : list of str\n",
    "        SMILES strings of the molecules seen before.\n",
    "    cost : int or float\n",
    "        Total monetary cost incurred at the current time.\n",
    "    number_molecules : int\n",
    "        Total number of molecules purchased at the current time.\n",
    "    time : int\n",
    "        Total number of days spent up to the current time.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, N, M, ensemble_size=3, epochs=1, molecule_cost=200,\n",
    "                 target_bounds={\"bace\":(4, math.inf), \"esol\":(-5, math.inf), \"logD\":(-0.4, 5.6)} ):\n",
    "        self.N = N #initial samples\n",
    "        self.M = M #batch size\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.epochs = epochs\n",
    "        self.molecule_cost = molecule_cost\n",
    "        self.target_bounds = target_bounds\n",
    "        \n",
    "        self.ensemble = {i:dc.models.GraphConvModel(n_tasks=3, mode='regression', batch_size=20, random_seed=i) \n",
    "                         for i in range(self.ensemble_size)} #map each model to its seed\n",
    "        self.history = [] #save snapshot of model, on disk\n",
    "        self.samples_seen = None\n",
    "        self.smiles_seen = []\n",
    "        self.cost = 0\n",
    "        self.number_molecules = 0\n",
    "        self.time = 0 #days\n",
    "        \n",
    "        \n",
    "    def train_model(self, model, dataset):\n",
    "        \"\"\"Helper function to train a given ensemble model on a given dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model : Keras model (generally deepchem.GraphConvModel)\n",
    "            Model to be trained.\n",
    "        dataset : pandas.DataFrame\n",
    "            Dataset to train on. Must include \"SMILES\", \"bace\", \"esol\", and \"logD\" headers.\n",
    "            \n",
    "        \"\"\"\n",
    "        #convert DataFrame to CSV and read in as deepchem.Dataset via deepchem.CSVLoader\n",
    "        dataset_temp_file = \"./temp/training_dataset.csv\"\n",
    "        dataset.to_csv(dataset_temp_file)\n",
    "\n",
    "        featurizer = dc.feat.ConvMolFeaturizer()\n",
    "        loader = dc.data.CSVLoader(tasks=[\"bace\", \"esol\", \"logD\"], smiles_field=\"SMILES\", featurizer=featurizer)\n",
    "\n",
    "        dataset_feat = loader.featurize(dataset_temp_file)\n",
    "\n",
    "        transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "        dataset_feat = transformer.transform(dataset_feat)\n",
    "\n",
    "        model.fit(dataset_feat, nb_epoch=1, deterministic=True, restore=False)\n",
    "    \n",
    "    \n",
    "    def train_ensemble(self, dataset):\n",
    "        \"\"\"Helper function to train model ensemble.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : pandas.Dataset\n",
    "            Dataset on which to train models. Must include \"SMILES\", \"bace\", \"esol\", and \"logD\" headers.\n",
    "        \n",
    "        \"\"\"\n",
    "        for model in self.ensemble.values():\n",
    "            self.train_model(model, dataset)\n",
    "\n",
    "    \n",
    "    def initial_training(self, verbose=False):\n",
    "        \"\"\"Train model ensemble for the first time on self.N samples randomly chosen from the 2500 lowest bace affinity-scored \n",
    "        molecules.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        verbose : bool\n",
    "            Whether to print progress updates.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        If self.N > 2500, ensemble will be trained on 2500 samples.\n",
    "        \n",
    "        \"\"\"\n",
    "        idx_range = self.N if self.N > low_bace_dataset.shape[0] else low_bace_dataset.shape[0]\n",
    "        rand_indices = random.sample(range(low_bace_dataset.shape[0]), k=idx_range) #select random row indices\n",
    "        \n",
    "        init_ensemble_dataset = pd.DataFrame()\n",
    "        for idx in rand_indices:\n",
    "            init_ensemble_dataset = init_ensemble_dataset.append( low_bace_dataset.iloc[idx], ignore_index=True )\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Training set selected.\")\n",
    "            \n",
    "        self.samples_seen = init_ensemble_dataset ### collect the examples seen during initial training\n",
    "        self.smiles_seen.extend( init_ensemble_dataset[\"SMILES\"].tolist() )\n",
    "        self.cost += 200 * len(init_ensemble_dataset)\n",
    "        self.number_molecules += len(init_ensemble_dataset)\n",
    "        self.time = 0 ### time to initially train? free initial knowledge?\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Training ensemble...\")\n",
    "            \n",
    "        self.train_ensemble(init_ensemble_dataset) #train ensemble on initial dataset\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Ensemble trained.\")\n",
    "        \n",
    "        \n",
    "    ###get_component_score, ket/list(keys)\n",
    "    def get_component_score(self, arr, keys):\n",
    "        \"\"\"Helper function to get the scaled \"goodness\" of the input scores.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        array : numpy.array\n",
    "             Array with bace, esol, and logD scores.\n",
    "        keys : collection of strings from {\"bace\", \"esol\", \"logD\"}\n",
    "            Which scores to incorporate into the overall goodness.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array\n",
    "            Sum of component scores.\n",
    "        \n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        if \"bace\" in keys:\n",
    "            #higher bace => higher score\n",
    "            bace = arr[:,0]\n",
    "            bace_range = self.target_bounds[\"bace\"]\n",
    "            scores.append( np.where(bace < bace_range[0], 0.2*bace-0.8, 0.05*bace-0.2) )\n",
    "            #dec penalty when score>low end of range\n",
    "        \n",
    "        if \"esol\" in keys:\n",
    "            esol = arr[:,1]\n",
    "            esol_range = self.target_bounds[\"esol\"]\n",
    "            scores.append( np.where(esol < esol_range[0], esol - np.absolute(esol-esol_range[1])**2, esol) )\n",
    "        \n",
    "        if \"logD\" in keys:\n",
    "            #logD within range is not penalized\n",
    "            logD = arr[:,2]\n",
    "            logD_range = self.target_bounds[\"logD\"]\n",
    "            #handle lower end of range\n",
    "            int_arr = np.where(logD < logD_range[0], logD - np.absolute(logD-logD_range[0]), logD)\n",
    "            #handle upper end of range\n",
    "            scores.append(np.where(int_arr > logD_range[1], int_arr - np.absolute(int_arr-logD_range[1]), int_arr) )\n",
    "\n",
    "        return sum(scores)\n",
    "        \n",
    "    \n",
    "    def score_and_select_top(self):\n",
    "        \"\"\"Scores all molecules and selects the top M for \"purchase\".\n",
    "        \n",
    "        \"\"\"\n",
    "        featurizer = dc.feat.ConvMolFeaturizer()\n",
    "        loader = dc.data.CSVLoader(tasks=[\"bace\", \"esol\", \"logD\"], smiles_field=\"SMILES\", featurizer=featurizer)\n",
    "        dataset_feat = loader.featurize(dataset_file) #featurize the molecules from the ground truth dataset\n",
    "        transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "        dataset_feat = transformer.transform(dataset_feat)\n",
    "        \n",
    "        predicted = np.zeros( (len(dataset_feat),3) )\n",
    "        for model in self.ensemble.values():\n",
    "            predicted += model.predict(dataset_feat)\n",
    "        predicted /= len(self.ensemble) #take the average of model predictions\n",
    "        \n",
    "        #copy SMILES and assign/calculate scores\n",
    "        results_df = pd.DataFrame()\n",
    "        results_df[\"SMILES\"] = ground_truth_dataset[\"SMILES\"]   \n",
    "\n",
    "        goodness = self.get_component_score(predicted, [\"bace\", \"esol\", \"logD\"])\n",
    "        results_df[\"bace\"] = predicted[:,0]\n",
    "        results_df[\"esol\"] = predicted[:,1]\n",
    "        results_df[\"logD\"] = predicted[:,2]\n",
    "        results_df[\"goodness\"] = goodness\n",
    "        \n",
    "        unseen_rows = results_df.loc[~results_df['SMILES'].isin(self.smiles_seen)] #remove examples previously seen\n",
    "        unseen_rows.sort_values(by=\"goodness\", ascending=False) #sort with highest goodness at top\n",
    "        \n",
    "        subset = unseen_rows[:self.M] if len(unseen_rows) > self.M else unseen_rows #select up to self.M samples\n",
    "        \n",
    "        self.samples_seen.append(subset, sort=False) # ignore_index=True?\n",
    "        self.cost += 200 * len(subset)\n",
    "        self.number_molecules += len(subset)\n",
    "            \n",
    "        self.time += 28 #4 weeks to buy and experiment\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"Simple wrapper to automate calls to select molecules and update models. \n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        Must be preceded by initial training of model ensemble.\n",
    "        \n",
    "        \"\"\"        \n",
    "        while len(self.samples_seen) < len(ground_truth_dataset): #replace with top bace score to exit -> 4.87(approx 95th %ile)\n",
    "            #find topmost mols\n",
    "            \n",
    "            #unseen_rows = results_df.loc[~results_df['SMILES'].isin(seen_smiles)] #remove examples previously seen\n",
    "                    \n",
    "            ###see if this works\n",
    "            candidates = self.samples_seen.loc[self.samples_seen['bace'] >= top_5_percent_bace_cutoff] #find mols w/ high bace\n",
    "            \n",
    "            esol_lower_bound = self.target_bounds[\"esol\"][0]\n",
    "            candidates = candidates.loc[candidates['esol'] >= esol_lower_bound] #filter the insoluble mols\n",
    "            \n",
    "            logD_range = self.target_bounds[\"logD\"]\n",
    "            candidates = candidates.loc[( candidates['logD'] >= logD_range[0] ) \n",
    "                                                 & ( candidates['logD'] <= logD_range[1] )] #filter for logD in range\n",
    "            \n",
    "            print(len(candidates))   \n",
    "            if len(candidates) > 0:\n",
    "                print(\"Molecule within bounds and 95th percentile bace affinity found.\")\n",
    "                return candidates\n",
    "                \n",
    "            self.score_and_select_top()\n",
    "            #record history\n",
    "            self.train_ensemble(self.samples_seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStep 1: load ground truth models and ensemble\\n\\nStep 2: train ensemble on N random data points (including ground truth values)\\n\\nStep 3: score all of the 10K molecules using the ensemble\\n\\nStep 4: take (\"buy\") the top M, and \"assess them experimentally\" (get their ground truth values)\\n\\nStep 5: add those samples to the training/seen set\\n\\nStep 6: retrain the ensemble\\n\\nStep 7: repeat (make 2-6 repeatable)\\n\\nStep 8: add some loops over N and M to generate plots of Hx vs N,M\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 1: load ground truth models and ensemble\n",
    "\n",
    "Step 2: train ensemble on N random data points (including ground truth values)\n",
    "\n",
    "Step 3: score all of the 10K molecules using the ensemble\n",
    "\n",
    "Step 4: take (\"buy\") the top M, and \"assess them experimentally\" (get their ground truth values)\n",
    "\n",
    "Step 5: add those samples to the training/seen set\n",
    "\n",
    "Step 6: retrain the ensemble\n",
    "\n",
    "Step 7: repeat (make 2-6 repeatable)\n",
    "\n",
    "Step 8: add some loops over N and M to generate plots of Hx vs N,M\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [10] #initial train set size\n",
    "M = [10] #batch size -> 96 wells, multiples\n",
    "for n in N:\n",
    "    for m in M:\n",
    "        E = Experimenter(n, m, ensemble_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 4.938 s\n",
      "TIMING: dataset construction took 6.436 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.059 s\n",
      "Loading dataset from disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darne\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending global_step 125: Average loss 0.7101\n",
      "TIMING: model fitting took 115.794 s\n"
     ]
    }
   ],
   "source": [
    "E.initial_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 19.477 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.293 s\n",
      "TIMING: dataset construction took 29.985 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 11.424 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 4.921 s\n",
      "TIMING: dataset construction took 6.948 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.721 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 250: Average loss 0.492426\n",
      "TIMING: model fitting took 43.919 s\n",
      "0\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 20.787 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.731 s\n",
      "TIMING: dataset construction took 32.474 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 11.068 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 5.339 s\n",
      "TIMING: dataset construction took 6.584 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 3.315 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 375: Average loss 0.410424\n",
      "TIMING: model fitting took 43.852 s\n",
      "0\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 20.232 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.532 s\n",
      "TIMING: dataset construction took 31.057 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 10.912 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 4.483 s\n",
      "TIMING: dataset construction took 5.714 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.462 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 500: Average loss 0.357851\n",
      "TIMING: model fitting took 41.049 s\n",
      "0\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 19.733 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.876 s\n",
      "TIMING: dataset construction took 30.567 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 11.723 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 4.972 s\n",
      "TIMING: dataset construction took 6.246 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.942 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 625: Average loss 0.314877\n",
      "TIMING: model fitting took 59.110 s\n",
      "0\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 19.968 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.477 s\n",
      "TIMING: dataset construction took 31.355 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 12.369 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 5.192 s\n",
      "TIMING: dataset construction took 6.338 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.569 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 750: Average loss 0.289852\n",
      "WARNING:tensorflow:From C:\\Users\\darne\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_management.py:624: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "TIMING: model fitting took 39.721 s\n",
      "0\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 51.392 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.308 s\n",
      "TIMING: dataset construction took 68.513 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 11.436 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 4.888 s\n",
      "TIMING: dataset construction took 6.034 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.635 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 875: Average loss 0.268173\n",
      "TIMING: model fitting took 49.805 s\n",
      "0\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 50.667 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.946 s\n",
      "TIMING: dataset construction took 67.510 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 13.704 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 4.842 s\n",
      "TIMING: dataset construction took 6.059 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.741 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 999: Average loss 0.253452\n",
      "Ending global_step 1000: Average loss 0.204501\n",
      "TIMING: model fitting took 54.693 s\n",
      "0\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 19.647 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.573 s\n",
      "TIMING: dataset construction took 29.112 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 13.693 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 4.734 s\n",
      "TIMING: dataset construction took 5.887 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.619 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 1125: Average loss 0.238095\n",
      "TIMING: model fitting took 53.393 s\n",
      "0\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 19.436 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.342 s\n",
      "TIMING: dataset construction took 29.745 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 13.372 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 6.218 s\n",
      "TIMING: dataset construction took 7.689 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.743 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 1250: Average loss 0.224794\n",
      "TIMING: model fitting took 47.217 s\n",
      "0\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 16.226 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.329 s\n",
      "TIMING: dataset construction took 25.744 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 13.868 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 3.584 s\n",
      "TIMING: dataset construction took 4.500 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.183 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 1375: Average loss 0.211608\n",
      "TIMING: model fitting took 45.945 s\n",
      "0\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 16.493 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.950 s\n",
      "TIMING: dataset construction took 24.731 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 30.380 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 3.883 s\n",
      "TIMING: dataset construction took 4.895 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.202 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 1500: Average loss 0.20144\n",
      "TIMING: model fitting took 39.878 s\n",
      "0\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 14.404 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.101 s\n",
      "TIMING: dataset construction took 22.930 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 9.616 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 3.567 s\n",
      "TIMING: dataset construction took 4.512 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.195 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 1625: Average loss 0.192005\n",
      "TIMING: model fitting took 37.492 s\n",
      "0\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 14.436 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 2.571 s\n",
      "TIMING: dataset construction took 22.382 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 10.471 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "TIMING: featurizing shard 0 took 4.105 s\n",
      "TIMING: dataset construction took 5.083 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 2.294 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 1750: Average loss 0.182836\n",
      "TIMING: model fitting took 36.423 s\n",
      "0\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n"
     ]
    }
   ],
   "source": [
    "E.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
