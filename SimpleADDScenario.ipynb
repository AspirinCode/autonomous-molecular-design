{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of dataset: ['Unnamed: 0' 'SMILES' 'esol' 'logD' 'bace']\n",
      "Number of examples in dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "###initialize imports and dataset\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import deepchem as dc\n",
    "from deepchem.utils.save import load_from_disk\n",
    "from deepchem.data import data_loader\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "dataset_file = \"./enamineSubset10KGroundTruth.csv\"\n",
    "ground_truth_dataset = load_from_disk(dataset_file) #pandas Dataframe\n",
    "\n",
    "low_bace_dataset = ground_truth_dataset.copy(deep=True).sort_values(by=\"bace\")[:2500] #take 2500 worst binders as starting set candidates\n",
    "\n",
    "print(\"Columns of dataset: %s\" % str(ground_truth_dataset.columns.values))\n",
    "print(\"Number of examples in dataset: %s\" % str(ground_truth_dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###initialize ground truth models and methods to access them\n",
    "\n",
    "def load_oracle_models():\n",
    "    bace_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/bace\")\n",
    "    esol_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/esol\")\n",
    "    logD_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/logD\")\n",
    "    bace_model.restore()\n",
    "    esol_model.restore()\n",
    "    logD_model.restore()\n",
    "    oracle = {\"bace\":bace_model, \"esol\":esol_model, \"logD\":logD_model} #get each model via the named property\n",
    "    return oracle\n",
    "    \n",
    "def query_oracle(smiles):\n",
    "    ### use when evaluating on the fly\n",
    "    raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model = dc.models.GraphConvModel(n_tasks=3, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/test_model\")\n",
    "\n",
    "def train_model(model, dataset):\n",
    "    #take in dataset as pandas Dataframe and convert to dc Dataset via pd to_csv and dc CSVLoader\n",
    "    dataset_temp_file = \"./temp/training_dataset.csv\"\n",
    "    dataset.to_csv(dataset_temp_file)\n",
    "    \n",
    "    featurizer = dc.feat.ConvMolFeaturizer()\n",
    "    loader = dc.data.CSVLoader(tasks=[\"bace\", \"logD\", \"esol\"], smiles_field=\"SMILES\", featurizer=featurizer)\n",
    "    \n",
    "    dataset_feat = loader.featurize(dataset_temp_file)\n",
    "\n",
    "    transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "    dataset_feat = transformer.transform(dataset_feat)\n",
    "    \n",
    "    model.fit(dataset_feat, nb_epoch=1, deterministic=True, restore=False)\n",
    "    #results = test_model.predict(dataset_feat)\n",
    "    \n",
    "#train_model(test_model, low_bace_dataset[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "###define Abstract Data Type to hold search information, including ensemble\n",
    "\n",
    "class Experimenter():\n",
    "    def __init__(self, N, M):\n",
    "        self.N = N #how many random samples to initially train on\n",
    "        self.M = M #the size of batch to purchase\n",
    "        self.ensemble = {i:dc.models.GraphConvModel(n_tasks=3, mode='regression', batch_size=20, random_seed=i) for i in range(3)} #map each model to its seed\n",
    "        self.history = [] #save snapshot of model, on disk\n",
    "        self.samples_seen = None\n",
    "        self.cost = 0\n",
    "        self.number_molecules = 0\n",
    "        self.time = 0 #days\n",
    "        self.training_epochs=1\n",
    "        self.ensemble_size = 3\n",
    "        self.cost_per_molecule = 200\n",
    "        self.target_bounds = {}\n",
    "              \n",
    "        self.bace_range = (4, math.inf)\n",
    "        self.esol_range = (-5, math.inf)\n",
    "        self.logD_range = (-0.4, 5.6)\n",
    "        \n",
    "    def train_model(self, model, dataset):\n",
    "        #take in dataset as pandas Dataframe and convert to dc Dataset via pd to_csv and dc CSVLoader\n",
    "        dataset_temp_file = \"./temp/training_dataset.csv\"\n",
    "        dataset.to_csv(dataset_temp_file)\n",
    "\n",
    "        featurizer = dc.feat.ConvMolFeaturizer()\n",
    "        loader = dc.data.CSVLoader(tasks=[\"bace\", \"logD\", \"esol\"], smiles_field=\"SMILES\", featurizer=featurizer)\n",
    "\n",
    "        dataset_feat = loader.featurize(dataset_temp_file)\n",
    "\n",
    "        transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "        dataset_feat = transformer.transform(dataset_feat)\n",
    "\n",
    "        model.fit(dataset_feat, nb_epoch=1, deterministic=True, restore=False)\n",
    "    \n",
    "    def train_ensemble(self, dataset):\n",
    "        for model in self.ensemble.values():\n",
    "            train_model(model, dataset)\n",
    "\n",
    "    \n",
    "    def initial_training(self):\n",
    "        ###train the ensemble on a subset that binds bace poorly\n",
    "        if self.N > low_bace_dataset.shape[0]:\n",
    "            self.N = low_bace_dataset.shape[0]\n",
    "        rand_indices = random.sample(range(low_bace_dataset.shape[0]), k=self.N) #select random row indices\n",
    "        \n",
    "        print(\"Random initial training indices selected.\")\n",
    "\n",
    "        init_ensemble_dataset = pd.DataFrame()\n",
    "        for idx in rand_indices:\n",
    "            init_ensemble_dataset = init_ensemble_dataset.append( low_bace_dataset.iloc[idx], ignore_index=True )\n",
    "        \n",
    "        print(\"Initial training dataset selected.\")\n",
    "        \n",
    "        self.samples_seen = init_ensemble_dataset ### collect the examples seen during initial training\n",
    "        self.cost += 200 * len(init_ensemble_dataset)\n",
    "        self.number_molecules += len(init_ensemble_dataset)\n",
    "        self.time = 0 ### time to initially train? free initial knowledge?\n",
    "        \n",
    "        print(\"Training ensemble...\")\n",
    "        self.train_ensemble(init_ensemble_dataset) #train ensemble on initialization\n",
    "        print(\"Ensemble trained.\")\n",
    "        \n",
    "        \n",
    "    ###get_component_score, ket/list(keys),\n",
    "    def get_bace_score(self, x):\n",
    "        #higher bace => higher score\n",
    "        return np.where(x < self.bace_range[0], 0.2*x-0.8, 0.05*x-0.2) #decrease penalty once score > lower end of range\n",
    "\n",
    "    def get_logD_score(self, x):\n",
    "        #logD within range is not penalized, \n",
    "        x = np.where(x < self.logD_range[0], x - np.absolute(x-self.logD_range[0]), x) #handle lower end of range\n",
    "        return np.where(x > self.logD_range[1], x - np.absolute(x-self.logD_range[1]), x) #handle upper end of range\n",
    "   \n",
    "    def get_esol_score(self, x):\n",
    "        return np.where(x < self.esol_range[0], x - np.absolute(x-self.logD_range[1])**2, x)\n",
    "        \n",
    "    def get_goodness_score(self, bace, logD, esol):\n",
    "        return self.get_bace_score(bace) + self.get_logD_score(logD) + self.get_esol_score(esol)\n",
    "    \n",
    "    \n",
    "    def score_and_select_top(self):\n",
    "\n",
    "        featurizer = dc.feat.ConvMolFeaturizer()\n",
    "        loader = dc.data.CSVLoader(tasks=[\"bace\", \"logD\", \"esol\"], smiles_field=\"SMILES\", featurizer=featurizer)\n",
    "        dataset_feat = loader.featurize(dataset_file)\n",
    "        transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "        dataset_feat = transformer.transform(dataset_feat)\n",
    "        \n",
    "        predicted = np.zeros( (len(dataset_feat),3) )\n",
    "        for model in self.ensemble.values():\n",
    "            predicted += model.predict(dataset_feat)\n",
    "        predicted /= len(self.ensemble) #take the average of model predictions\n",
    "        \n",
    "        #copy SMILES and assign/calculate scores\n",
    "        results_df = pd.DataFrame()\n",
    "        results_df[\"SMILES\"] = ground_truth_dataset[\"SMILES\"]   \n",
    "        bace = predicted[:,0]\n",
    "        logD = predicted[:,1]\n",
    "        esol = predicted[:,2]\n",
    "        goodness = self.get_goodness_score(bace, logD, esol)\n",
    "        results_df[\"bace\"] = bace\n",
    "        results_df[\"logD\"] = logD\n",
    "        results_df[\"esol\"] = esol\n",
    "        results_df[\"goodness\"] = goodness\n",
    "        \n",
    "        seen_smiles = self.samples_seen[\"SMILES\"].tolist()\n",
    "        \n",
    "        unseen_rows = results_df.loc[~results_df['SMILES'].isin(seen_smiles)] #remove examples previously seen\n",
    "        unseen_rows.sort_values(by=\"goodness\", ascending=False) #sort with highest goodness at top\n",
    "        \n",
    "        if len(unseen_rows) > self.M:\n",
    "            subset = unseen_rows[:self.M]\n",
    "        else:\n",
    "            subset = unseen_rows\n",
    "        \n",
    "        self.samples_seen.append(subset, sort=False) # ignore_index=True?\n",
    "        self.cost += 200 * len(subset)\n",
    "        self.number_molecules += len(subset)\n",
    "            \n",
    "        self.time += 28 #4 weeks to buy and experiment\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        while len(self.samples_seen) < len(ground_truth_dataset): #replace with top bace score to exit\n",
    "            self.score_and_select_top()\n",
    "            #record history\n",
    "            self.train_ensemble(self.samples_seen)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStep 1: load ground truth models and ensemble\\n\\nStep 2: train ensemble on N random data points (including ground truth values)\\n\\nStep 3: score all of the 10K molecules using the ensemble\\n\\nStep 4: take (\"buy\") the top M, and \"assess them experimentally\" (get their ground truth values)\\n\\nStep 5: add those samples to the training/seen set\\n\\nStep 6: retrain the ensemble\\n\\nStep 7: repeat (make 2-6 repeatable)\\n\\nStep 8: add some loops over N and M to generate plots of Hx vs N,M\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Step 1: load ground truth models and ensemble\n",
    "\n",
    "Step 2: train ensemble on N random data points (including ground truth values)\n",
    "\n",
    "Step 3: score all of the 10K molecules using the ensemble\n",
    "\n",
    "Step 4: take (\"buy\") the top M, and \"assess them experimentally\" (get their ground truth values)\n",
    "\n",
    "Step 5: add those samples to the training/seen set\n",
    "\n",
    "Step 6: retrain the ensemble\n",
    "\n",
    "Step 7: repeat (make 2-6 repeatable)\n",
    "\n",
    "Step 8: add some loops over N and M to generate plots of Hx vs N,M\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random initial training indices selected.\n",
      "Initial training dataset selected.\n",
      "Training ensemble...\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.020 s\n",
      "TIMING: dataset construction took 0.090 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.048 s\n",
      "Loading dataset from disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darne\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending global_step 1: Average loss 2.22575\n",
      "TIMING: model fitting took 46.918 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.018 s\n",
      "TIMING: dataset construction took 0.084 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.037 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 1: Average loss 0.666458\n",
      "TIMING: model fitting took 74.107 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.063 s\n",
      "TIMING: dataset construction took 0.149 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.058 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 1: Average loss 0.841572\n",
      "TIMING: model fitting took 52.685 s\n",
      "Ensemble trained.\n"
     ]
    }
   ],
   "source": [
    "N = [10] #initial train set size\n",
    "M = [10] #batch size -> 96 wells, multiples\n",
    "for n in N:\n",
    "    for m in M:\n",
    "        E = Experimenter(n, m)\n",
    "        E.initial_training()\n",
    "        #e.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 19.532 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.887 s\n",
      "TIMING: dataset construction took 31.272 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 7.466 s\n",
      "Loading dataset from disk.\n",
      "                                                 SMILES      bace      logD  \\\n",
      "0               COC1=CC(C)=CC=C1CN(CC1=CC=CC=C1F)CC(C)C -0.569863  0.089220   \n",
      "1     CCOC1CC(N(C)C2=NN=C(C3=CC=C(C4=CC=C(F)C=C4)S3)... -0.770531 -0.305879   \n",
      "2      CC(C(=O)NC1=CC=NC(C(=O)OC(C)(C)C)=C1)C1CCC(=O)N1 -0.957855  0.102194   \n",
      "3              O=C(NC1=CC(F)=CC=C1OC1=CC=CC=C1)N1CCCCO1 -0.524457  0.035249   \n",
      "4        COC(=O)C1=CC2=CC(NC(=O)CCC3=NN=C[NH]3)=CC=C2S1 -0.600162  0.081840   \n",
      "5                 O=C(CNC(=O)NC1CCOC2(CCOCC2)C1)NCC(F)F -0.984144  0.180185   \n",
      "6        O=C1C2=C(N=CN1CC=CC1=CC=C(Br)C=C1)N(C1CC1)N=N2 -0.373227 -0.067187   \n",
      "7     COC(=O)C1=CC2=CC=CC(NC(=O)N3CC(C)OC4(CCOC4C)C3... -0.755964  0.007497   \n",
      "8     CC1=C(C(=O)N2CCC(CN(C)C(=O)C3=NN(C)C(C)=C3Cl)C... -0.522791  0.226833   \n",
      "9     COC1=CC=C(CCC2=NN=C(C(C)(C)NC(=O)OC(C)(C)C)O2)... -0.832511 -0.001997   \n",
      "10           CC(C)C[C@@H]1C[C@H]1NS(=O)(=O)CC1(CC#N)CC1 -1.071434  0.218942   \n",
      "11            CN(C)C1=NC=C(C(=O)N(C)C2=C[NH]N=C2)C=C1Cl -0.542045 -0.094612   \n",
      "12    O=C(CC1=CSC=C1)NCC1CCN(C2CCCN(C3=CC=CC=C3)C2=O)C1 -0.520704  0.104042   \n",
      "13             COC1=CC(Br)=CC(NC(=O)NCC(=O)N2CCNCC2)=C1 -0.435116  0.014664   \n",
      "14    CN(C)C1=NC2=CC(NCC3=CC=C4C=CC(F)=C(F)C4=N3)=CC... -0.360985 -0.212897   \n",
      "15    CCC1=C(C)C=C(C(=O)NCC2(O)CN(C(C)C3=CC=C(F)C=C3... -0.662767  0.019193   \n",
      "16    CC(C)(C)OC(=O)N1C[C@H]2[C@@H](C1)[C@@H]2C(=O)N... -1.014443  0.254472   \n",
      "17    N#CC(C1=CC=CC(Cl)=C1)N1CCN(CC2=CC=C(Cl)C3=CC=C... -0.536716  0.126819   \n",
      "18                        CCSC1=CC=CC(CNC(CC)(CC)CO)=C1 -0.834211  0.106802   \n",
      "19    CN1C(COC2=CC=C(Cl)C=C2)=NN=C1SC1CCN(C2=CC(OC(F... -0.743844  0.001350   \n",
      "20           CC1=CC=C(N(C)CC2=CN3C=CC=C(Br)C3=N2)C=C1Cl -0.469431 -0.049189   \n",
      "21         CC1=C2C=C(C(=O)N3C(C)CNC(C)C3C)[NH]C2=CC=C1F -0.528046  0.071063   \n",
      "22          CC1=CC=CC2=NC(CN3C=C(C4(O)CCCCC4)N=N3)=CN12 -0.738573  0.078048   \n",
      "23    CC1=CC=CC(C2=CCN(C(=O)C3=CC(S(=O)(=O)N4CCOCC4)... -0.700825  0.107785   \n",
      "24      CN(C)C(=O)C1=CC=C(NC(=O)C2=CC=NC(C3CC3)=C2)C=N1 -0.373907 -0.040950   \n",
      "25    NC1=NC(CN(CC2=CC=CC=C2)CC(O)C2=CSC=C2)=NC(NC2=... -0.627918  0.058167   \n",
      "26    CC(C)(C)OC(=O)N1CCC(CC(=O)N2CCOC3=CC=NC=C32)C2... -0.930751  0.040770   \n",
      "27    CCN(CC1CCN(C(=O)CC2(C)CC2)CC1)C(=O)C1=NSC2=CC=... -0.817323  0.023294   \n",
      "28     CCC(OC1=CC=CC=C1C)C(=O)NCC1(NC(=O)C2=CCCC2)CCCC1 -0.857953  0.227890   \n",
      "29        COC1=CC=C(C2=N[NH]C=C2C(=O)NC(C)C2CCCNC2)C=C1 -0.584563  0.203837   \n",
      "...                                                 ...       ...       ...   \n",
      "9970  CCC1=CN=CC(C(=O)N2C[C@@H]3C[C@H]2CN3C(=O)CN2N=... -0.376559  0.010330   \n",
      "9971  CNS(=O)(=O)C1=CC(C(=O)N(CC2=CC=CC=C2)C(C)C2=CC... -0.684526  0.043537   \n",
      "9972    CCC(CC)C(C(=O)NCCN1C=CC2=CC(Cl)=CC=C21)N1CCOCC1 -0.491068  0.201744   \n",
      "9973  CCC1=NC(CN2CCCC(NC3=C4C5=C(CCCC5)SC4=NC(C4CC4)... -0.485400 -0.010598   \n",
      "9974  CN(CCN(C)C(=O)[C@@H]1OC[C@@H]2COCC[C@@H]21)C(=... -0.532913  0.007912   \n",
      "9975            CC1=CC=CC2=C1CCN2C(=O)NCCS(=O)CC(F)(F)F -0.571741  0.096279   \n",
      "9976         CC(CC(C)NC1=NC=CN=C1F)NC(=O)CN1N=CN(C)C1=O -0.582151 -0.163469   \n",
      "9977  CCC(C)OC1=CC(C(=O)OCC(=O)C2=CC3=CC=CC=C3O2)=CC=N1 -0.432898 -0.110566   \n",
      "9978  CN(CCN(C)C(=O)C1CCN2C=NC=C2C1)C(=O)CNC(=O)C1=C... -0.475345  0.195859   \n",
      "9979  CCN(CC(F)(F)F)C(=O)CSC1=NN=C(CC2=CC=C(OC)C=C2)N1N -0.663521 -0.067622   \n",
      "9980  COCC(C)(C)CC(=O)NCC(O)CNC(=O)C1=CC=C(C2=C[NH]C... -0.866558  0.200842   \n",
      "9981           CCC1=NC(O)=CC(CNC(C)C2=CC(Br)=CC=C2C)=N1 -0.322018  0.020881   \n",
      "9982  CNS(=O)(=O)C1=CC(C(=O)OC[C@@H]2C[C@H]2C2=CC=C(... -0.601467  0.028532   \n",
      "9983  CCCOC(C(=O)NC1=C2NC(=O)COC2=CC(Br)=C1)C1=CC=C(... -0.399211 -0.042768   \n",
      "9984  CCCN(C)S(=O)(=O)NC(=O)C1=C(SC2=CC=CC=C2)N(C)N=C1C -0.672542  0.067142   \n",
      "9985     N#CCC1=CC=C(C(=O)OC(CC(F)(F)F)C2=CC=CC=C2)C=C1 -0.651028  0.058921   \n",
      "9986  COC(=O)C1=CN([C@@H]2CCCC[C@@H]2NC(=O)C2=NOC(C3... -0.588627  0.120259   \n",
      "9987               CC1=NN(C)C=C1CNC1=CC=C(C(C)(C)C)N=C1 -0.911626  0.106730   \n",
      "9988  CCCNC(=O)NC(C)(C)C(=O)NCC1(NC(=O)C2CN(C)C(=O)N... -0.961409  0.074327   \n",
      "9989      O=C(COC(=O)C1C2C3CC4C5C3CC2C5C41)C1=CC=C[NH]1 -0.548968  0.144416   \n",
      "9990  CC1=NN=CC=C1C(=O)NCC1CCN(C(=O)CC(C)C2=CC=CC(F)... -0.518916  0.142914   \n",
      "9991  COC1=CC=C([C@@H]2CN(C3=CC(NC4CC4)=NC=N3)C[C@H]... -0.546419  0.047593   \n",
      "9992                CN1CCCC1CCNC1=CC(Cl)=C2C=NN(C)C2=N1 -0.514356 -0.001152   \n",
      "9993                      CCC(CNCC1CC1C)OC1=CC=CC(F)=C1 -0.506314  0.121900   \n",
      "9994          COCC1CCN1C(=O)N[C@@H]1CS(=O)(=O)C[C@@H]1C -0.793543  0.367537   \n",
      "9995       CC1=C(Br)C=NN1CC(=O)OC(C)C(=O)NCCC1=CC=CC=C1 -0.451261 -0.038566   \n",
      "9996  CC(=O)N1CC2=CC=C(S(=O)(=O)NCCC(C)N(C)CC3=CC=CC... -0.605115 -0.011634   \n",
      "9997  CCCN(C(=O)CC1=NN=C(C)O1)C1CCN(C(=O)C2=CC=C(Cl)... -0.378827 -0.055232   \n",
      "9998  COCC(NC(=O)C(CC(C)(C)C)N(C)C(=O)OC(C)(C)C)(C(=... -0.936635  0.155077   \n",
      "9999    CCC1=NN=C(N(CC(C)C)C(C)C)N1CC1=CC=C(OC(C)C)N=C1 -0.482043 -0.010429   \n",
      "\n",
      "          esol  goodness  \n",
      "0    -0.438292 -1.263045  \n",
      "1    -0.641159 -1.901143  \n",
      "2    -0.671914 -1.561291  \n",
      "3    -0.387915 -1.257557  \n",
      "4    -0.457648 -1.295841  \n",
      "5    -0.216972 -1.033616  \n",
      "6    -0.593149 -1.534982  \n",
      "7    -0.483601 -1.427297  \n",
      "8    -0.674622 -1.352347  \n",
      "9    -0.689475 -1.657974  \n",
      "10   -0.318943 -1.114288  \n",
      "11   -0.618814 -1.621835  \n",
      "12   -0.557212 -1.357310  \n",
      "13   -0.459299 -1.331658  \n",
      "14   -0.453651 -1.538745  \n",
      "15   -0.460036 -1.373396  \n",
      "16   -0.785197 -1.533613  \n",
      "17   -0.577233 -1.357757  \n",
      "18   -0.300721 -1.160761  \n",
      "19   -0.600860 -1.548278  \n",
      "20   -0.513903 -1.456978  \n",
      "21   -0.183926 -1.018472  \n",
      "22   -0.566932 -1.436598  \n",
      "23   -0.646913 -1.479292  \n",
      "24   -0.620872 -1.536604  \n",
      "25   -0.657966 -1.525382  \n",
      "26   -0.823704 -1.769084  \n",
      "27   -0.582494 -1.522665  \n",
      "28   -0.545301 -1.289002  \n",
      "29   -0.423616 -1.136692  \n",
      "...        ...       ...  \n",
      "9970 -0.751961 -1.616942  \n",
      "9971 -0.730724 -1.624093  \n",
      "9972 -0.462617 -1.159087  \n",
      "9973 -0.680780 -1.588459  \n",
      "9974 -0.260983 -1.159654  \n",
      "9975 -0.522449 -1.340518  \n",
      "9976 -0.523768 -1.603667  \n",
      "9977 -0.422237 -1.419382  \n",
      "9978 -0.497152 -1.196361  \n",
      "9979 -0.617755 -1.618081  \n",
      "9980 -0.349851 -1.122321  \n",
      "9981 -0.388043 -1.231567  \n",
      "9982 -0.646146 -1.537908  \n",
      "9983 -0.396948 -1.319558  \n",
      "9984 -0.671008 -1.538375  \n",
      "9985 -0.439342 -1.310627  \n",
      "9986 -0.509320 -1.306786  \n",
      "9987 -0.581883 -1.457478  \n",
      "9988 -0.349987 -1.267942  \n",
      "9989 -0.507522 -1.272900  \n",
      "9990 -0.557137 -1.318007  \n",
      "9991 -0.696635 -1.558326  \n",
      "9992 -0.475720 -1.379743  \n",
      "9993 -0.284302 -1.063665  \n",
      "9994 -0.296489 -0.887661  \n",
      "9995 -0.486221 -1.415040  \n",
      "9996 -0.744498 -1.677155  \n",
      "9997 -0.428460 -1.359458  \n",
      "9998 -0.598576 -1.430826  \n",
      "9999 -0.635135 -1.541972  \n",
      "\n",
      "[10000 rows x 5 columns]\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.078 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darne\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6201: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMING: dataset construction took 0.231 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.099 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 2: Average loss 1.35298\n",
      "TIMING: model fitting took 8.796 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.020 s\n",
      "TIMING: dataset construction took 0.085 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.065 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 2: Average loss 0.472056\n",
      "TIMING: model fitting took 14.877 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.025 s\n",
      "TIMING: dataset construction took 0.086 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.048 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 2: Average loss 0.532098\n",
      "TIMING: model fitting took 15.196 s\n"
     ]
    }
   ],
   "source": [
    "E.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
