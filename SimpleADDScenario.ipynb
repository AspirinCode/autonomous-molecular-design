{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns of dataset: ['Unnamed: 0' 'SMILES' 'esol' 'logD' 'bace']\n",
      "Number of examples in dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "###initialize imports and dataset\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import deepchem as dc\n",
    "from deepchem.utils.save import load_from_disk\n",
    "from deepchem.data import data_loader\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "dataset_file = \"./enamineSubset10KGroundTruth.csv\"\n",
    "ground_truth_dataset = load_from_disk(dataset_file) #pandas Dataframe\n",
    "\n",
    "low_bace_dataset = ground_truth_dataset.copy(deep=True).sort_values(by=\"bace\")[:2500] #take 2500 worst binders as starting set candidates\n",
    "\n",
    "print(\"Columns of dataset: %s\" % str(ground_truth_dataset.columns.values))\n",
    "print(\"Number of examples in dataset: %s\" % str(ground_truth_dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###initialize ground truth models and methods to access them\n",
    "\n",
    "def load_oracle_models():\n",
    "    bace_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/bace\")\n",
    "    esol_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/esol\")\n",
    "    logD_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/logD\")\n",
    "    bace_model.restore()\n",
    "    esol_model.restore()\n",
    "    logD_model.restore()\n",
    "    oracle = {\"bace\":bace_model, \"esol\":esol_model, \"logD\":logD_model} #get each model via the named property\n",
    "    return oracle\n",
    "    \n",
    "def query_oracle(smiles):\n",
    "    ### use when evaluating on the fly\n",
    "    raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model = dc.models.GraphConvModel(n_tasks=3, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/test_model\")\n",
    "\n",
    "def train_model(model, dataset):\n",
    "    #take in dataset as pandas Dataframe and convert to dc Dataset via pd to_csv and dc CSVLoader\n",
    "    dataset_temp_file = \"./temp/training_dataset.csv\"\n",
    "    dataset.to_csv(dataset_temp_file)\n",
    "    \n",
    "    featurizer = dc.feat.ConvMolFeaturizer()\n",
    "    loader = dc.data.CSVLoader(tasks=[\"bace\", \"logD\", \"esol\"], smiles_field=\"SMILES\", featurizer=featurizer)\n",
    "    \n",
    "    dataset_feat = loader.featurize(dataset_temp_file)\n",
    "\n",
    "    transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "    dataset_feat = transformer.transform(dataset_feat)\n",
    "    \n",
    "    model.fit(dataset_feat, nb_epoch=1, deterministic=True, restore=False)\n",
    "    #results = test_model.predict(dataset_feat)\n",
    "    \n",
    "#train_model(test_model, low_bace_dataset[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "###define Abstract Data Type to hold search information, including ensemble\n",
    "\n",
    "class Experimenter():\n",
    "    def __init__(self, N, M):\n",
    "        self.N = N #how many random samples to initially train on\n",
    "        self.M = M #the size of batch to purchase\n",
    "        self.ensemble = {i:dc.models.GraphConvModel(n_tasks=3, mode='regression', batch_size=20, random_seed=i) for i in range(3)} #map each model to its seed\n",
    "        self.history = [] #save snapshot of model, on disk\n",
    "        self.samples_seen = None\n",
    "        self.cost = 0\n",
    "        self.number_molecules = 0\n",
    "        self.time = 0 #days\n",
    "        self.training_epochs=1\n",
    "        self.ensemble_size = 3\n",
    "        self.cost_per_molecule = 200\n",
    "        self.target_bounds = {}\n",
    "              \n",
    "        self.bace_range = (4, math.inf)\n",
    "        self.esol_range = (-5, math.inf)\n",
    "        self.logD_range = (-0.4, 5.6)\n",
    "        \n",
    "    def train_model(self, model, dataset):\n",
    "        #take in dataset as pandas Dataframe and convert to dc Dataset via pd to_csv and dc CSVLoader\n",
    "        dataset_temp_file = \"./temp/training_dataset.csv\"\n",
    "        dataset.to_csv(dataset_temp_file)\n",
    "\n",
    "        featurizer = dc.feat.ConvMolFeaturizer()\n",
    "        loader = dc.data.CSVLoader(tasks=[\"bace\", \"logD\", \"esol\"], smiles_field=\"SMILES\", featurizer=featurizer)\n",
    "\n",
    "        dataset_feat = loader.featurize(dataset_temp_file)\n",
    "\n",
    "        transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "        dataset_feat = transformer.transform(dataset_feat)\n",
    "\n",
    "        model.fit(dataset_feat, nb_epoch=1, deterministic=True, restore=False)\n",
    "    \n",
    "    def train_ensemble(self, dataset):\n",
    "        for model in self.ensemble.values():\n",
    "            train_model(model, dataset)\n",
    "\n",
    "    \n",
    "    def initial_training(self):\n",
    "        ###train the ensemble on a subset that binds bace poorly\n",
    "        if self.N > low_bace_dataset.shape[0]:\n",
    "            self.N = low_bace_dataset.shape[0]\n",
    "        rand_indices = random.sample(range(low_bace_dataset.shape[0]), k=self.N) #select random row indices\n",
    "        \n",
    "        print(\"Random initial training indices selected.\")\n",
    "\n",
    "        init_ensemble_dataset = pd.DataFrame()\n",
    "        for idx in rand_indices:\n",
    "            init_ensemble_dataset = init_ensemble_dataset.append( low_bace_dataset.iloc[idx], ignore_index=True )\n",
    "        \n",
    "        print(\"Initial training dataset selected.\")\n",
    "        \n",
    "        self.samples_seen = init_ensemble_dataset ### collect the examples seen during initial training\n",
    "        self.cost += 200 * len(init_ensemble_dataset)\n",
    "        self.number_molecules += len(init_ensemble_dataset)\n",
    "        self.time = 0 ### time to initially train? free initial knowledge?\n",
    "        \n",
    "        print(\"Training ensemble...\")\n",
    "        self.train_ensemble(init_ensemble_dataset) #train ensemble on initialization\n",
    "        print(\"Ensemble trained.\")\n",
    "        \n",
    "        \n",
    "    ###get_component_score, ket/list(keys),\n",
    "    def get_bace_score(self, x):\n",
    "        #higher bace => higher score\n",
    "        return np.where(x < self.bace_range[0], 0.2*x-0.8, 0.05*x-0.2) #decrease penalty once score > lower end of range\n",
    "\n",
    "    def get_logD_score(self, x):\n",
    "        #logD within range is not penalized, \n",
    "        x = np.where(x < self.logD_range[0], x - np.absolute(x-self.logD_range[0]), x) #handle lower end of range\n",
    "        return np.where(x > self.logD_range[1], x - np.absolute(x-self.logD_range[1]), x) #handle upper end of range\n",
    "   \n",
    "    def get_esol_score(self, x):\n",
    "        return np.where(x < self.esol_range[0], x - np.absolute(x-self.logD_range[1])**2, x)\n",
    "        \n",
    "    def get_goodness_score(self, bace, logD, esol):\n",
    "        return get_bace_score(bace) + get_logD_score(logD) + get_esol_score(esol)\n",
    "    \n",
    "    \n",
    "    def score_and_select_top(self):\n",
    "        predicted = np.zeroes( (len(ground_truth_dataset),3) )\n",
    "        for model in self.ensemble.values():\n",
    "            predicted += model.predict(ground_truth_dataset)\n",
    "        predicted /= len(self.ensemble) #take the average of model predictions\n",
    "        \n",
    "        #copy SMILES and assign/calculate scores\n",
    "        results_df = pd.DataFrame()\n",
    "        results_df[\"SMILES\"] = ground_truth_dataset[\"SMILES\"]   \n",
    "        bace = predicted[:,0]\n",
    "        logD = predicted[:,1]\n",
    "        esol = predicted[:,2]\n",
    "        goodness = self.get_goodness_score(bace, logD, esol)\n",
    "        results_df[\"bace\"] = bace\n",
    "        results_df[\"logD\"] = logD\n",
    "        results_df[\"esol\"] = esol\n",
    "        results_df[\"goodness\"] = goodness\n",
    "        print(results_df)\n",
    "        \n",
    "        seen_smiles = self.samples_seen[\"SMILES\"].tolist()\n",
    "        \n",
    "        unseen_rows = results_df.loc[~df['SMILES'].isin(seen_smiles)] #remove examples previously seen\n",
    "        unseen_rows.sort_values(by=\"goodness\", ascending=False) #sort with highest goodness at top\n",
    "        \n",
    "        if len(unseen_rows) > self.M:\n",
    "            subset = unseen_rows[:self.M]\n",
    "        else:\n",
    "            subset = unseen_rows\n",
    "        \n",
    "        self.samples_seen.append(subset) # ignore_index=True?\n",
    "        self.cost += 200 * len(subset)\n",
    "        self.number_molecules += len(subset)\n",
    "            \n",
    "        self.time += 28 #4 weeks to buy and experiment\n",
    "        \n",
    "        \n",
    "    def run(self):\n",
    "        while len(self.seen_samples) < len(ground_truth_dataset): #replace with top bace score to exit\n",
    "            self.score_and_select_top()\n",
    "            #record history\n",
    "            self.train_ensemble(self.samples_seen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1: load ground truth models and ensemble\n",
    "\n",
    "Step 2: train ensemble on N random data points (including ground truth values)\n",
    "\n",
    "Step 3: score all of the 10K molecules using the ensemble\n",
    "\n",
    "Step 4: take (\"buy\") the top M, and \"assess them experimentally\" (get their ground truth values)\n",
    "\n",
    "Step 5: add those samples to the training/seen set\n",
    "\n",
    "Step 6: retrain the ensemble\n",
    "\n",
    "Step 7: repeat (make 2-6 repeatable)\n",
    "\n",
    "Step 8: add some loops over N and M to generate plots of Hx vs N,M\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random initial training indices selected.\n",
      "Initial training dataset selected.\n",
      "Training ensemble...\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 3.280 s\n",
      "TIMING: dataset construction took 4.054 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 1.290 s\n",
      "Loading dataset from disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darne\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending global_step 100: Average loss 0.729629\n",
      "TIMING: model fitting took 426.686 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 7.687 s\n",
      "TIMING: dataset construction took 10.141 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 3.146 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 100: Average loss 0.710652\n",
      "TIMING: model fitting took 476.719 s\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 0 took 8.983 s\n",
      "TIMING: dataset construction took 11.217 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 4.688 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 100: Average loss 0.674514\n",
      "TIMING: model fitting took 559.385 s\n",
      "Ensemble trained.\n"
     ]
    }
   ],
   "source": [
    "N = [2000] #initial train set size\n",
    "M = [5000] #batch size -> 96 wells, multiples\n",
    "for n in N:\n",
    "    for m in M:\n",
    "        E = Experimenter(n, m)\n",
    "        E.initial_training()\n",
    "        #e.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
