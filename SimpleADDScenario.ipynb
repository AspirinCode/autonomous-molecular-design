{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff bace score for 95th percentile: 4.870083\n",
      "Columns of dataset: ['Index' 'SMILES' 'bace' 'esol' 'logD']\n",
      "Number of examples in dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "###initialize imports and dataset\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import deepchem as dc\n",
    "from deepchem.utils.save import load_from_disk\n",
    "from deepchem.data import data_loader\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "dataset_file = \"./enamineSubset10KGroundTruth.csv\"\n",
    "ground_truth_dataset = load_from_disk(dataset_file) #pandas Dataframe\n",
    "\n",
    "low_bace_dataset = ground_truth_dataset.sort_values(by=\"bace\")[:2500] #take 2.5K worst binder potential starters,shouldn't need copy\n",
    "\n",
    "top_5_percent_index = len(ground_truth_dataset) // 20\n",
    "top_5_percent_bace_cutoff = ground_truth_dataset.sort_values(by=\"bace\", ascending=True,)[\"bace\"][top_5_percent_index]\n",
    "\n",
    "print(\"Cutoff bace score for 95th percentile:\", top_5_percent_bace_cutoff)\n",
    "print(\"Columns of dataset: %s\" % str(ground_truth_dataset.columns.values))\n",
    "print(\"Number of examples in dataset: %s\" % str(ground_truth_dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###initialize ground truth models and methods to access them\n",
    "\n",
    "def load_oracle_models():\n",
    "    \"\"\"Loads the pretrained ground truth models for evaluating molecules' properties on-the-fly.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    oracle : dict\n",
    "        A dictionary containing models mapped to their property keywords: \"bace\", \"esol\", \"logD\".\n",
    "    \"\"\"\n",
    "    bace_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/bace\")\n",
    "    esol_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/esol\")\n",
    "    logD_model = dc.models.GraphConvModel(n_tasks=1, mode='regression', batch_size=50, random_seed=0, model_dir=\"./models/logD\")\n",
    "    bace_model.restore()\n",
    "    esol_model.restore()\n",
    "    logD_model.restore()\n",
    "    oracle = {\"bace\":bace_model, \"esol\":esol_model, \"logD\":logD_model} #get each model via the named property\n",
    "    return oracle\n",
    "\n",
    "def query_oracle(dataset, oracle):\n",
    "    \"\"\"Evaluate molecules on-the-fly for their estimated bace, esol, and logD scores.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : pandas.DataFrame\n",
    "        The input dataset; must includes a field with smiles strings under keyword \"SMILES\".\n",
    "    oracle : dictionary( dc.models.GraphConvModel )\n",
    "        The pretrained ground truth value prediction models.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    results : pandas.DataFrame\n",
    "        Copy of input dataset with newly estimated bace, esol, and logD scores under those headers. \n",
    "    \"\"\"\n",
    "    query_file = \"./temp/oracle_eval.csv\"\n",
    "    dataset.to_csv(query_file)\n",
    "    \n",
    "    results = dataset.copy(deep=True) #defensive copy of input dataframe \n",
    "    \n",
    "    featurizer = dc.feat.ConvMolFeaturizer()\n",
    "    for prop in (\"bace\", \"esol\", \"logD\"):\n",
    "        #retrieve appropriate model from oracle\n",
    "        model = oracle[prop]\n",
    "        \n",
    "        #load, featurize, and normalize input dataset\n",
    "        loader = dc.data.CSVLoader(tasks=[prop], smiles_field=\"SMILES\",featurizer=featurizer)\n",
    "        dataset_feat = loader.featurize(query_file)\n",
    "        transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "        dataset_feat = transformer.transform(dataset_feat)\n",
    "        \n",
    "        #predict and assign property results to keyword\n",
    "        predicted = model.predict(dataset_feat)\n",
    "        results[prop] = predicted\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "###define Abstract Data Type to hold search information, including ensemble\n",
    "\n",
    "class Experimenter():\n",
    "    \"\"\"Class representing a research scientist/team going through the drug development process.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        Number of samples to initially train the experimenter ensemble on.\n",
    "    M : int\n",
    "        Number of molecules to purchase in each batch.\n",
    "    ensemble_size : int, optional\n",
    "        Number of models in experimenter ensemble.\n",
    "    epochs : int, optional\n",
    "        Number of epochs to train ensemble models for at each stage.\n",
    "    molecule_cost : int or float, optional\n",
    "        Monetary cost of purchasing a single molecule.\n",
    "    target_bounds : dictionary of str:tuples(floats), optional\n",
    "        Desired range for each property.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    ensemble : dictionary of deepchem.models.GrachConvModel\n",
    "        Models representing the experimenter knowledge/predictions and uncertainty.\n",
    "    history : list of <NEEDS IMPLEMENTING>\n",
    "        Snapshots of the model state at each time step.\n",
    "    samples_seen : pandas.DataFrame\n",
    "        All of the molecules seen before. Includes initial training set.\n",
    "    smiles_seen : list of str\n",
    "        SMILES strings of the molecules seen before.\n",
    "    cost : int or float\n",
    "        Total monetary cost incurred at the current time.\n",
    "    number_molecules : int\n",
    "        Total number of molecules purchased at the current time.\n",
    "    time : int\n",
    "        Total number of days spent up to the current time.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, N, M, ensemble_size=3, epochs=1, molecule_cost=200,\n",
    "                 target_bounds={\"bace\":(4, math.inf), \"esol\":(-5, math.inf), \"logD\":(-0.4, 5.6)} ):\n",
    "        self.N = N #initial samples\n",
    "        self.M = M #batch size\n",
    "        self.ensemble_size = ensemble_size\n",
    "        self.epochs = epochs\n",
    "        self.molecule_cost = molecule_cost\n",
    "        self.target_bounds = target_bounds\n",
    "        \n",
    "        self.ensemble = {i:dc.models.GraphConvModel(n_tasks=3, mode='regression', batch_size=20, random_seed=i) \n",
    "                         for i in range(self.ensemble_size)} #map each model to its seed\n",
    "        self.history = [] #save snapshot of model, on disk\n",
    "        self.samples_seen = None\n",
    "        self.smiles_seen = []\n",
    "        self.cost = 0\n",
    "        self.number_molecules = 0\n",
    "        self.time = 0 #days\n",
    "        \n",
    "        \n",
    "    def train_model(self, model, dataset):\n",
    "        \"\"\"Helper function to train a given ensemble model on a given dataset.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model : Keras model (generally deepchem.GraphConvModel)\n",
    "            Model to be trained.\n",
    "        dataset : pandas.DataFrame\n",
    "            Dataset to train on. Must include \"SMILES\", \"bace\", \"esol\", and \"logD\" headers.\n",
    "            \n",
    "        \"\"\"\n",
    "        #convert DataFrame to CSV and read in as deepchem.Dataset via deepchem.CSVLoader\n",
    "        dataset_temp_file = \"./temp/training_dataset.csv\"\n",
    "        dataset.to_csv(dataset_temp_file)\n",
    "\n",
    "        featurizer = dc.feat.ConvMolFeaturizer()\n",
    "        loader = dc.data.CSVLoader(tasks=[\"bace\", \"esol\", \"logD\"], smiles_field=\"SMILES\", featurizer=featurizer)\n",
    "\n",
    "        dataset_feat = loader.featurize(dataset_temp_file)\n",
    "\n",
    "        transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "        dataset_feat = transformer.transform(dataset_feat)\n",
    "\n",
    "        model.fit(dataset_feat, nb_epoch=1, deterministic=True, restore=False)\n",
    "    \n",
    "    \n",
    "    def train_ensemble(self, dataset):\n",
    "        \"\"\"Helper function to train model ensemble.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : pandas.Dataset\n",
    "            Dataset on which to train models. Must include \"SMILES\", \"bace\", \"esol\", and \"logD\" headers.\n",
    "        \n",
    "        \"\"\"\n",
    "        for model in self.ensemble.values():\n",
    "            self.train_model(model, dataset)\n",
    "\n",
    "    \n",
    "    def initial_training(self, verbose=False):\n",
    "        \"\"\"Train model ensemble for the first time on self.N samples randomly chosen from the 2500 lowest bace affinity-scored \n",
    "        molecules.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        verbose : bool\n",
    "            Whether to print progress updates.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        If self.N > 2500, ensemble will be trained on 2500 samples.\n",
    "        Records first history object.\n",
    "        \n",
    "        \"\"\"\n",
    "        idx_range = self.N if self.N < low_bace_dataset.shape[0] else low_bace_dataset.shape[0]\n",
    "        rand_indices = random.sample(range(low_bace_dataset.shape[0]), k=idx_range) #select random row indices\n",
    "        \n",
    "        init_ensemble_dataset = pd.DataFrame()\n",
    "        for idx in rand_indices:\n",
    "            init_ensemble_dataset = init_ensemble_dataset.append( low_bace_dataset.iloc[idx], ignore_index=True )\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Training set selected.\")\n",
    "            \n",
    "        self.samples_seen = init_ensemble_dataset ### collect the examples seen during initial training\n",
    "        self.smiles_seen = init_ensemble_dataset[\"SMILES\"].tolist()\n",
    "        \n",
    "        #cost/time to initially train? free initial knowledge?\n",
    "        self.cost += 200 * len(init_ensemble_dataset)\n",
    "        self.number_molecules += len(init_ensemble_dataset)\n",
    "        self.time = 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Training ensemble...\")\n",
    "            \n",
    "        self.train_ensemble(init_ensemble_dataset) #train ensemble on initial dataset\n",
    "        \n",
    "        self.record_history()\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Ensemble trained.\")\n",
    "        \n",
    "        \n",
    "    ###get_component_score, ket/list(keys)\n",
    "    def get_component_score(self, arr, keys):\n",
    "        \"\"\"Helper function to get the scaled \"goodness\" of the input scores.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        array : numpy.array\n",
    "             Array with bace, esol, and logD scores.\n",
    "        keys : collection of strings from {\"bace\", \"esol\", \"logD\"}\n",
    "            Which scores to incorporate into the overall goodness.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        numpy.array\n",
    "            Sum of component scores.\n",
    "        \n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        if \"bace\" in keys:\n",
    "            #higher bace => higher score\n",
    "            bace = arr[:,0]\n",
    "            bace_range = self.target_bounds[\"bace\"]\n",
    "            scores.append( np.where(bace < bace_range[0], 0.2*bace-0.8, 0.05*bace-0.2) )\n",
    "            #dec penalty when score>low end of range\n",
    "        \n",
    "        if \"esol\" in keys:\n",
    "            esol = arr[:,1]\n",
    "            esol_range = self.target_bounds[\"esol\"]\n",
    "            scores.append( np.where(esol < esol_range[0], esol - np.absolute(esol-esol_range[1])**2, esol) )\n",
    "        \n",
    "        if \"logD\" in keys:\n",
    "            #logD within range is not penalized\n",
    "            logD = arr[:,2]\n",
    "            logD_range = self.target_bounds[\"logD\"]\n",
    "            #handle lower end of range\n",
    "            int_arr = np.where(logD < logD_range[0], logD - np.absolute(logD-logD_range[0]), logD)\n",
    "            #handle upper end of range\n",
    "            scores.append(np.where(int_arr > logD_range[1], int_arr - np.absolute(int_arr-logD_range[1]), int_arr) )\n",
    "\n",
    "        return sum(scores)\n",
    "        \n",
    "    \n",
    "    def score_and_select_top(self):\n",
    "        \"\"\"Scores all molecules and selects the top M for \"purchase\".\n",
    "        \n",
    "        \"\"\"\n",
    "        featurizer = dc.feat.ConvMolFeaturizer()\n",
    "        loader = dc.data.CSVLoader(tasks=[\"bace\", \"esol\", \"logD\"], smiles_field=\"SMILES\", featurizer=featurizer)\n",
    "        dataset_feat = loader.featurize(dataset_file) #featurize the molecules from the ground truth dataset\n",
    "        transformer = dc.trans.NormalizationTransformer(transform_y=True, dataset=dataset_feat)\n",
    "        dataset_feat = transformer.transform(dataset_feat)\n",
    "        \n",
    "        predicted = np.zeros( (len(dataset_feat),3) )\n",
    "        for model in self.ensemble.values():\n",
    "            predicted += model.predict(dataset_feat)\n",
    "        predicted /= len(self.ensemble) #take the average of model predictions\n",
    "        \n",
    "        #copy SMILES and assign/calculate scores\n",
    "        results_df = pd.DataFrame()\n",
    "        results_df[\"SMILES\"] = ground_truth_dataset[\"SMILES\"]   \n",
    "\n",
    "        goodness = self.get_component_score(predicted, [\"bace\", \"esol\", \"logD\"])\n",
    "        results_df[\"bace\"] = predicted[:,0]\n",
    "        results_df[\"esol\"] = predicted[:,1]\n",
    "        results_df[\"logD\"] = predicted[:,2]\n",
    "        results_df[\"goodness\"] = goodness\n",
    "        \n",
    "        unseen_rows = results_df.loc[~results_df['SMILES'].isin(self.smiles_seen)] #remove examples previously seen\n",
    "        unseen_rows = unseen_rows.sort_values(by=\"goodness\", ascending=False) #sort with highest goodness at top\n",
    "        \n",
    "        subset = unseen_rows[:self.M] if len(unseen_rows) > self.M else unseen_rows #select up to self.M samples\n",
    "        \n",
    "        self.samples_seen = pd.concat([self.samples_seen,subset], sort=False)\n",
    "        self.smiles_seen = self.samples_seen[\"SMILES\"].tolist()\n",
    "        #self.samples_seen = self.samples_seen.append(subset, sort=False) # ignore_index=True?\n",
    "        \n",
    "        self.cost += 200 * len(subset)\n",
    "        self.number_molecules += len(subset)\n",
    "            \n",
    "        self.time += 28 #4 weeks to buy and experiment\n",
    "        \n",
    "    \n",
    "    def record_history(self):\n",
    "        \"\"\"Stores model costs and experience for later analysis.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        Does not save self.history attribute, in order to avoid redundantly storing the data in it.\n",
    "        Only saves attributes that change in each time step.\n",
    "        \n",
    "        \"\"\"\n",
    "        hist = {}\n",
    "        hist[\"samples_seen\"] = self.samples_seen\n",
    "        hist[\"smiles_seen\"] = self.smiles_seen\n",
    "        hist[\"cost\"] = self.cost\n",
    "        hist[\"number_molecules\"] = self.number_molecules\n",
    "        hist[\"time\"] = self.time\n",
    "        self.history.append(hist)\n",
    "     \n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Simple wrapper to automate calls to select molecules and update models. \n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        Must be preceded by initial training of model ensemble.\n",
    "        \n",
    "        \"\"\"        \n",
    "        while len(self.samples_seen) < len(ground_truth_dataset): #replace with top bace score to exit -> 4.87(approx 95th %ile)\n",
    "            candidates = self.samples_seen.loc[self.samples_seen['bace'] >= top_5_percent_bace_cutoff] #find mols w/ high bace\n",
    "            \n",
    "            esol_lower_bound = self.target_bounds[\"esol\"][0]\n",
    "            candidates = candidates.loc[candidates['esol'] >= esol_lower_bound] #filter the insoluble mols\n",
    "            \n",
    "            logD_range = self.target_bounds[\"logD\"]\n",
    "            candidates = candidates.loc[( candidates['logD'] >= logD_range[0] ) \n",
    "                                                 & ( candidates['logD'] <= logD_range[1] )] #filter for logD in range\n",
    "              \n",
    "            if len(candidates) > 0:\n",
    "                print(\"Molecule within bounds and 95th percentile bace affinity found.\")\n",
    "                return candidates\n",
    "                \n",
    "            self.score_and_select_top()\n",
    "            self.record_history()\n",
    "            self.train_ensemble(self.samples_seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 1: load ground truth models and ensemble\n",
    "\n",
    "Step 2: train ensemble on N random data points (including ground truth values)\n",
    "\n",
    "Step 3: score all of the 10K molecules using the ensemble\n",
    "\n",
    "Step 4: take (\"buy\") the top M, and \"assess them experimentally\" (get their ground truth values)\n",
    "\n",
    "Step 5: add those samples to the training/seen set\n",
    "\n",
    "Step 6: retrain the ensemble\n",
    "\n",
    "Step 7: repeat (make 2-6 repeatable)\n",
    "\n",
    "Step 8: add some loops over N and M to generate plots of Hx vs N,M\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [10] #initial train set size\n",
    "M = [10] #batch size -> 96 wells, multiples\n",
    "for n in N:\n",
    "    for m in M:\n",
    "        E = Experimenter(n, m, ensemble_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.020 s\n",
      "TIMING: dataset construction took 0.085 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.042 s\n",
      "Loading dataset from disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darne\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending global_step 1: Average loss 1.02542\n",
      "TIMING: model fitting took 22.758 s\n"
     ]
    }
   ],
   "source": [
    "print(E.history)\n",
    "E.initial_training()\n",
    "E.record_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./enamineSubset10KGroundTruth.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "Featurizing sample 4000\n",
      "Featurizing sample 5000\n",
      "Featurizing sample 6000\n",
      "Featurizing sample 7000\n",
      "Featurizing sample 8000\n",
      "TIMING: featurizing shard 0 took 18.032 s\n",
      "Loading shard 2 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "TIMING: featurizing shard 1 took 3.135 s\n",
      "TIMING: dataset construction took 26.323 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 7.636 s\n",
      "Loading dataset from disk.\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from ./temp/training_dataset.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "TIMING: featurizing shard 0 took 0.035 s\n",
      "TIMING: dataset construction took 0.107 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.052 s\n",
      "Loading dataset from disk.\n",
      "Ending global_step 2: Average loss 1.10158\n",
      "TIMING: model fitting took 2.491 s\n",
      "\n",
      "\n",
      " [{'samples_seen':     Index                                             SMILES      bace  \\\n",
      "0  4569.0  CCCNC(=O)C1CCCN(C(=O)C(C)(C)C2=CC=C(Br)C(C)=C2)C1  3.716476   \n",
      "1  8630.0                  COCCC(C)(C)C(=O)N1CC2(CCOCC2)CC1C  3.588901   \n",
      "2  4317.0  COC(=O)C1=C(C)OC(S(=O)(=O)N2CCC(CC(C)(C)C)C2(C...  3.596402   \n",
      "3  6818.0            CC(C)OC1CC(N2CCC(C(C)(O)C(F)(F)F)CC2)C1  3.593647   \n",
      "4  8540.0                   CC1=CC=C(CC2=NC(C(F)(F)F)=NO2)S1  3.155709   \n",
      "5  1259.0     CC(C)(NC(=O)C1=CC=C(Cl)C(C(F)(F)F)=C1)C1CCOCC1  3.686496   \n",
      "6  9611.0           COC(=O)CC(NC(=O)C1=NN(CCN(C)C)C=C1)C(C)C  3.877279   \n",
      "7  4057.0                     CCC(C)(CC)C(=O)NC(C)CCC(C)(C)C  3.336584   \n",
      "8  4762.0           CC(C)C1=NN=C(CCC(=O)N(C)CCCS(C)(=O)=O)O1  3.399246   \n",
      "9  2282.0                     CCC(CC#N)NC(=O)C(C)(C)N1CCOCC1  3.497454   \n",
      "\n",
      "       esol      logD  \n",
      "0 -4.380369  0.424986  \n",
      "1 -3.686151  0.941184  \n",
      "2 -4.538015  0.410045  \n",
      "3 -4.413987  0.883511  \n",
      "4 -3.957097  0.892982  \n",
      "5 -4.390866 -0.068839  \n",
      "6 -4.576400  0.213405  \n",
      "7 -4.117688  0.105239  \n",
      "8 -4.074825  0.508130  \n",
      "9 -4.081919  0.284380  , 'smiles_seen': ['CCCNC(=O)C1CCCN(C(=O)C(C)(C)C2=CC=C(Br)C(C)=C2)C1', 'COCCC(C)(C)C(=O)N1CC2(CCOCC2)CC1C', 'COC(=O)C1=C(C)OC(S(=O)(=O)N2CCC(CC(C)(C)C)C2(C)C)=C1', 'CC(C)OC1CC(N2CCC(C(C)(O)C(F)(F)F)CC2)C1', 'CC1=CC=C(CC2=NC(C(F)(F)F)=NO2)S1', 'CC(C)(NC(=O)C1=CC=C(Cl)C(C(F)(F)F)=C1)C1CCOCC1', 'COC(=O)CC(NC(=O)C1=NN(CCN(C)C)C=C1)C(C)C', 'CCC(C)(CC)C(=O)NC(C)CCC(C)(C)C', 'CC(C)C1=NN=C(CCC(=O)N(C)CCCS(C)(=O)=O)O1', 'CCC(CC#N)NC(=O)C(C)(C)N1CCOCC1'], 'cost': 2000, 'number_molecules': 10, 'time': 0}, {'samples_seen':        Index                                             SMILES      bace  \\\n",
      "0     4569.0  CCCNC(=O)C1CCCN(C(=O)C(C)(C)C2=CC=C(Br)C(C)=C2)C1  3.716476   \n",
      "1     8630.0                  COCCC(C)(C)C(=O)N1CC2(CCOCC2)CC1C  3.588901   \n",
      "2     4317.0  COC(=O)C1=C(C)OC(S(=O)(=O)N2CCC(CC(C)(C)C)C2(C...  3.596402   \n",
      "3     6818.0            CC(C)OC1CC(N2CCC(C(C)(O)C(F)(F)F)CC2)C1  3.593647   \n",
      "4     8540.0                   CC1=CC=C(CC2=NC(C(F)(F)F)=NO2)S1  3.155709   \n",
      "5     1259.0     CC(C)(NC(=O)C1=CC=C(Cl)C(C(F)(F)F)=C1)C1CCOCC1  3.686496   \n",
      "6     9611.0           COC(=O)CC(NC(=O)C1=NN(CCN(C)C)C=C1)C(C)C  3.877279   \n",
      "7     4057.0                     CCC(C)(CC)C(=O)NC(C)CCC(C)(C)C  3.336584   \n",
      "8     4762.0           CC(C)C1=NN=C(CCC(=O)N(C)CCCS(C)(=O)=O)O1  3.399246   \n",
      "9     2282.0                     CCC(CC#N)NC(=O)C(C)(C)N1CCOCC1  3.497454   \n",
      "8545     NaN               O=C(OCC1=C(Cl)SC(Cl)=C1)C12CCCN1CCC2 -0.796027   \n",
      "9766     NaN  CC1=CC(C(=O)NC2CN(C(=O)C3C(C)(C)C3(C)C)CC2C)=C... -0.518850   \n",
      "4405     NaN              COC1=CC=C2SC(CNC3(C4CCCO4)CC3)=CC2=C1 -0.692559   \n",
      "9804     NaN            CC1=NN(CC2=CN=C(C(C)(C)C)N=C2)C(C)=C1Br -0.848983   \n",
      "5530     NaN  CC1=CN(C2=C(C(=O)N3CC4(CCOCC4)C3C(C)C)C(C)=NN2... -1.237446   \n",
      "8607     NaN                   C1=NC2=CC=C(NC3CCC34CCCC4)C=C2S1 -1.138304   \n",
      "8025     NaN              CC1=C(C(=O)N2CCNCC23CC3)OC2=CC=CC=C12 -0.573896   \n",
      "9480     NaN                CC(C)N1CCN(C(=O)C2(N(C)C)CCC2)CC1=O -0.849951   \n",
      "5205     NaN  C1=CC=C([C@@H]2C[C@H]2CN2C=NC(C3=CC=CN=C3)=C2)... -0.981182   \n",
      "9585     NaN         CN(CC1=N[NH]C(C(C)(C)C)=N1)CC1=CC(Cl)=CN1C -0.797591   \n",
      "\n",
      "          esol      logD  goodness  \n",
      "0    -4.380369  0.424986       NaN  \n",
      "1    -3.686151  0.941184       NaN  \n",
      "2    -4.538015  0.410045       NaN  \n",
      "3    -4.413987  0.883511       NaN  \n",
      "4    -3.957097  0.892982       NaN  \n",
      "5    -4.390866 -0.068839       NaN  \n",
      "6    -4.576400  0.213405       NaN  \n",
      "7    -4.117688  0.105239       NaN  \n",
      "8    -4.074825  0.508130       NaN  \n",
      "9    -4.081919  0.284380       NaN  \n",
      "8545  0.527767  1.271648  0.840209  \n",
      "9766  0.616565  1.119756  0.832551  \n",
      "4405  0.587794  1.170878  0.820161  \n",
      "9804  0.586297  1.179127  0.795627  \n",
      "5530  0.312547  1.504408  0.769466  \n",
      "8607  0.498143  1.279714  0.750195  \n",
      "8025  0.328301  1.282813  0.696335  \n",
      "9480  0.360556  1.283498  0.674065  \n",
      "5205  0.371790  1.297932  0.673486  \n",
      "9585  0.595969  1.032111  0.668562  , 'smiles_seen': ['CCCNC(=O)C1CCCN(C(=O)C(C)(C)C2=CC=C(Br)C(C)=C2)C1', 'COCCC(C)(C)C(=O)N1CC2(CCOCC2)CC1C', 'COC(=O)C1=C(C)OC(S(=O)(=O)N2CCC(CC(C)(C)C)C2(C)C)=C1', 'CC(C)OC1CC(N2CCC(C(C)(O)C(F)(F)F)CC2)C1', 'CC1=CC=C(CC2=NC(C(F)(F)F)=NO2)S1', 'CC(C)(NC(=O)C1=CC=C(Cl)C(C(F)(F)F)=C1)C1CCOCC1', 'COC(=O)CC(NC(=O)C1=NN(CCN(C)C)C=C1)C(C)C', 'CCC(C)(CC)C(=O)NC(C)CCC(C)(C)C', 'CC(C)C1=NN=C(CCC(=O)N(C)CCCS(C)(=O)=O)O1', 'CCC(CC#N)NC(=O)C(C)(C)N1CCOCC1', 'O=C(OCC1=C(Cl)SC(Cl)=C1)C12CCCN1CCC2', 'CC1=CC(C(=O)NC2CN(C(=O)C3C(C)(C)C3(C)C)CC2C)=C(C)S1', 'COC1=CC=C2SC(CNC3(C4CCCO4)CC3)=CC2=C1', 'CC1=NN(CC2=CN=C(C(C)(C)C)N=C2)C(C)=C1Br', 'CC1=CN(C2=C(C(=O)N3CC4(CCOCC4)C3C(C)C)C(C)=NN2C)N=C1', 'C1=NC2=CC=C(NC3CCC34CCCC4)C=C2S1', 'CC1=C(C(=O)N2CCNCC23CC3)OC2=CC=CC=C12', 'CC(C)N1CCN(C(=O)C2(N(C)C)CCC2)CC1=O', 'C1=CC=C([C@@H]2C[C@H]2CN2C=NC(C3=CC=CN=C3)=C2)C=C1', 'CN(CC1=N[NH]C(C(C)(C)C)=N1)CC1=CC(Cl)=CN1C'], 'cost': 4000, 'number_molecules': 20, 'time': 28}]\n"
     ]
    }
   ],
   "source": [
    "E.run()\n",
    "print(\"\\n\\n\",E.history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
